package main

// æ‰€æœ‰å¯¼å…¥å·²æ³¨é‡Šï¼Œå¦‚éœ€è¿è¡Œè¯·å–æ¶ˆæ³¨é‡Šmainå‡½æ•°å’Œç›¸å…³å¯¼å…¥
/*
import (
	"context"
	"fmt"
	"log"
	"runtime"
	"sync"
	"sync/atomic"
	"time"

	"github.com/UTC-Six/pool/worker_pool"
)
*/

// å¦‚éœ€è¿è¡Œå‹åŠ›æµ‹è¯•ï¼Œè¯·å–æ¶ˆæ³¨é‡Šä¸‹é¢çš„mainå‡½æ•°
/*
func main() {
	fmt.Println("ğŸš€ Worker Pool ç”Ÿäº§ç¯å¢ƒå‹åŠ›æµ‹è¯•")
	fmt.Printf("ç³»ç»Ÿä¿¡æ¯: CPUæ ¸å¿ƒæ•°=%d, GOMAXPROCS=%d\n", runtime.NumCPU(), runtime.GOMAXPROCS(0))

	// è¿è¡Œæ‰€æœ‰æµ‹è¯•
	runBasicStressTest()
	runHighConcurrencyTest()
	runLongRunningTest()
	runResourceLeakTest()
	runCoreWorkersAdjustmentTest()
	runErrorHandlingTest()
	runContextCancellationTest()

	fmt.Println("âœ… æ‰€æœ‰å‹åŠ›æµ‹è¯•å®Œæˆï¼")
}

// åŸºç¡€å‹åŠ›æµ‹è¯•
func runBasicStressTest() {
	fmt.Println("\n--- åŸºç¡€å‹åŠ›æµ‹è¯• ---")

	pool := worker_pool.NewPool(20,
		worker_pool.WithMaxWorkers(100),
		worker_pool.WithName("stress-test-pool"),
		worker_pool.WithLogger(log.Printf),
	)
	defer pool.Shutdown()

	numTasks := 10000
	var completed int64
	var errors int64
	var wg sync.WaitGroup

	start := time.Now()

	for i := 0; i < numTasks; i++ {
		wg.Add(1)
		err := pool.Submit(context.Background(), func(ctx context.Context) (interface{}, error) {
			defer wg.Done()

			// æ¨¡æ‹Ÿä¸åŒç±»å‹çš„å·¥ä½œè´Ÿè½½
			workType := atomic.AddInt64(&completed, 1) % 3
			switch workType {
			case 0:
				// CPUå¯†é›†å‹
				sum := 0
				for j := 0; j < 10000; j++ {
					sum += j
				}
				return sum, nil
			case 1:
				// IOå¯†é›†å‹ï¼ˆæ¨¡æ‹Ÿï¼‰
				time.Sleep(1 * time.Millisecond)
				return "io-result", nil
			case 2:
				// æ··åˆå‹
				time.Sleep(500 * time.Microsecond)
				sum := 0
				for j := 0; j < 1000; j++ {
					sum += j
				}
				return sum, nil
			}
			return nil, nil
		})

		if err != nil {
			atomic.AddInt64(&errors, 1)
		}
	}

	wg.Wait()
	duration := time.Since(start)

	stats := pool.EnhancedStats()
	tps := float64(numTasks) / duration.Seconds()

	fmt.Printf("âœ… åŸºç¡€å‹åŠ›æµ‹è¯•å®Œæˆ:\n")
	fmt.Printf("   - ä»»åŠ¡æ•°: %d\n", numTasks)
	fmt.Printf("   - å®Œæˆæ•°: %d\n", completed)
	fmt.Printf("   - é”™è¯¯æ•°: %d\n", errors)
	fmt.Printf("   - ç”¨æ—¶: %v\n", duration)
	fmt.Printf("   - TPS: %.2f\n", tps)
	fmt.Printf("   - æœ€ç»ˆç»Ÿè®¡: Active=%d, Completed=%d, Total=%d\n",
		stats.ActiveWorkers, stats.Completed, stats.TaskSubmitCount)

	if errors > 0 {
		fmt.Printf("âš ï¸  å‘ç° %d ä¸ªé”™è¯¯\n", errors)
	}
	if tps < 500 {
		fmt.Printf("âš ï¸  æ€§èƒ½è¾ƒä½: %.2f TPS (æœŸæœ› > 500)\n", tps)
	}
}

// é«˜å¹¶å‘æµ‹è¯•
func runHighConcurrencyTest() {
	fmt.Println("\n--- é«˜å¹¶å‘æµ‹è¯• ---")

	pool := worker_pool.NewPool(10,
		worker_pool.WithMaxWorkers(200),
		worker_pool.WithName("concurrency-test-pool"),
	)
	defer pool.Shutdown()

	numGoroutines := 100
	tasksPerGoroutine := 100
	var totalCompleted int64
	var wg sync.WaitGroup

	start := time.Now()

	// å¯åŠ¨å¤šä¸ªgoroutineå¹¶å‘æäº¤ä»»åŠ¡
	for i := 0; i < numGoroutines; i++ {
		wg.Add(1)
		go func(goroutineID int) {
			defer wg.Done()

			var localWg sync.WaitGroup
			for j := 0; j < tasksPerGoroutine; j++ {
				localWg.Add(1)
				err := pool.Submit(context.Background(), func(ctx context.Context) (interface{}, error) {
					defer localWg.Done()
					atomic.AddInt64(&totalCompleted, 1)
					// éšæœºå·¥ä½œè´Ÿè½½
					if goroutineID%2 == 0 {
						time.Sleep(100 * time.Microsecond)
					} else {
						sum := 0
						for k := 0; k < 1000; k++ {
							sum += k
						}
					}
					return nil, nil
				})

				if err != nil {
					fmt.Printf("âŒ Goroutine %d æäº¤ä»»åŠ¡å¤±è´¥: %v\n", goroutineID, err)
					localWg.Done() // å¦‚æœæäº¤å¤±è´¥ï¼Œæ‰‹åŠ¨è°ƒç”¨Done
				}
			}
			localWg.Wait() // ç­‰å¾…è¯¥goroutineçš„æ‰€æœ‰ä»»åŠ¡å®Œæˆ
		}(i)
	}

	wg.Wait()
	duration := time.Since(start)

	expectedTasks := int64(numGoroutines * tasksPerGoroutine)
	stats := pool.EnhancedStats()

	fmt.Printf("âœ… é«˜å¹¶å‘æµ‹è¯•å®Œæˆ:\n")
	fmt.Printf("   - å¹¶å‘æ•°: %d goroutines\n", numGoroutines)
	fmt.Printf("   - æ¯ä¸ªgoroutineä»»åŠ¡æ•°: %d\n", tasksPerGoroutine)
	fmt.Printf("   - é¢„æœŸä»»åŠ¡æ•°: %d\n", expectedTasks)
	fmt.Printf("   - å®é™…å®Œæˆæ•°: %d\n", totalCompleted)
	fmt.Printf("   - ç”¨æ—¶: %v\n", duration)
	fmt.Printf("   - TPS: %.2f\n", float64(expectedTasks)/duration.Seconds())
	fmt.Printf("   - Workeræ‰©å®¹æƒ…å†µ: Min=%d, Max=%d, å½“å‰Active=%d\n",
		stats.MinWorkers, stats.MaxWorkers, stats.ActiveWorkers)

	if totalCompleted != expectedTasks {
		fmt.Printf("âŒ ä»»åŠ¡å®Œæˆæ•°ä¸åŒ¹é…: æœŸæœ› %d, å®é™… %d\n", expectedTasks, totalCompleted)
	}
}

// é•¿æ—¶é—´è¿è¡Œæµ‹è¯•
func runLongRunningTest() {
	fmt.Println("\n--- é•¿æ—¶é—´è¿è¡Œæµ‹è¯• ---")

	pool := worker_pool.NewPool(5,
		worker_pool.WithMaxWorkers(20),
		worker_pool.WithName("long-running-test"),
		worker_pool.WithAdjustCheckInterval(1*time.Second), // å¿«é€Ÿè°ƒæ•´ç”¨äºæµ‹è¯•
		worker_pool.WithCoreAdjustStrategy(worker_pool.StrategyPercentage),
		worker_pool.WithLowLoadThreshold(0.3),
	)
	defer pool.Shutdown()

	var taskCounter int64
	stopCh := make(chan struct{})

	// å¯åŠ¨ä»»åŠ¡æäº¤å™¨
	go func() {
		ticker := time.NewTicker(10 * time.Millisecond)
		defer ticker.Stop()

		for {
			select {
			case <-ticker.C:
				pool.Submit(context.Background(), func(ctx context.Context) (interface{}, error) {
					atomic.AddInt64(&taskCounter, 1)
					time.Sleep(5 * time.Millisecond)
					return nil, nil
				})
			case <-stopCh:
				return
			}
		}
	}()

	// å¯åŠ¨ç»Ÿè®¡ç›‘æ§
	go func() {
		ticker := time.NewTicker(2 * time.Second)
		defer ticker.Stop()

		for {
			select {
			case <-ticker.C:
				stats := pool.EnhancedStats()
				fmt.Printf("   [ç›‘æ§] Tasks=%d, Active=%d, Core=%d, Queue=%d, History=%d\n",
					atomic.LoadInt64(&taskCounter), stats.ActiveWorkers, stats.CoreWorkers,
					stats.QueuedTasks, stats.LoadHistoryLength)
			case <-stopCh:
				return
			}
		}
	}()

	// è¿è¡Œ10ç§’
	fmt.Println("   è¿è¡Œ10ç§’é•¿æ—¶é—´æµ‹è¯•...")
	time.Sleep(10 * time.Second)
	close(stopCh)

	// ç­‰å¾…å‰©ä½™ä»»åŠ¡å®Œæˆ
	time.Sleep(1 * time.Second)

	finalStats := pool.EnhancedStats()
	fmt.Printf("âœ… é•¿æ—¶é—´è¿è¡Œæµ‹è¯•å®Œæˆ:\n")
	fmt.Printf("   - æ€»ä»»åŠ¡æ•°: %d\n", atomic.LoadInt64(&taskCounter))
	fmt.Printf("   - æœ€ç»ˆç»Ÿè®¡: Active=%d, Core=%d, Completed=%d\n",
		finalStats.ActiveWorkers, finalStats.CoreWorkers, finalStats.Completed)
	fmt.Printf("   - è´Ÿè½½å†å²é•¿åº¦: %d\n", finalStats.LoadHistoryLength)
}

// èµ„æºæ³„éœ²æµ‹è¯•
func runResourceLeakTest() {
	fmt.Println("\n--- èµ„æºæ³„éœ²æµ‹è¯• ---")

	var m1, m2 runtime.MemStats
	runtime.GC()
	runtime.ReadMemStats(&m1)

	initialGoroutines := runtime.NumGoroutine()

	// åˆ›å»ºå’Œé”€æ¯å¤šä¸ªæ± 
	for i := 0; i < 50; i++ {
		pool := worker_pool.NewPool(3,
			worker_pool.WithMaxWorkers(10),
			worker_pool.WithAdjustCheckInterval(100*time.Millisecond),
		)

		// æäº¤ä¸€äº›ä»»åŠ¡
		for j := 0; j < 20; j++ {
			pool.Submit(context.Background(), func(ctx context.Context) (interface{}, error) {
				time.Sleep(1 * time.Millisecond)
				return nil, nil
			})
		}

		time.Sleep(50 * time.Millisecond)
		pool.Shutdown()
	}

	// ç­‰å¾…èµ„æºæ¸…ç†
	time.Sleep(500 * time.Millisecond)
	runtime.GC()
	runtime.ReadMemStats(&m2)

	finalGoroutines := runtime.NumGoroutine()
	var memoryIncrease int64
	if m2.Alloc >= m1.Alloc {
		memoryIncrease = int64(m2.Alloc - m1.Alloc)
	} else {
		memoryIncrease = -int64(m1.Alloc - m2.Alloc) // å†…å­˜å®é™…å‡å°‘äº†
	}
	goroutineIncrease := finalGoroutines - initialGoroutines

	fmt.Printf("âœ… èµ„æºæ³„éœ²æµ‹è¯•å®Œæˆ:\n")
	fmt.Printf("   - åˆå§‹Goroutines: %d\n", initialGoroutines)
	fmt.Printf("   - æœ€ç»ˆGoroutines: %d\n", finalGoroutines)
	fmt.Printf("   - Goroutineå¢é•¿: %d\n", goroutineIncrease)
	fmt.Printf("   - å†…å­˜å˜åŒ–: %d bytes (%.2f KB)\n", memoryIncrease, float64(memoryIncrease)/1024)

	if goroutineIncrease > 10 {
		fmt.Printf("âš ï¸  å¯èƒ½å­˜åœ¨Goroutineæ³„éœ²: å¢é•¿äº† %d ä¸ª\n", goroutineIncrease)
	}
	if memoryIncrease > 5*1024*1024 { // 5MB
		fmt.Printf("âš ï¸  å¯èƒ½å­˜åœ¨å†…å­˜æ³„éœ²: å¢é•¿äº† %.2f MB\n", float64(memoryIncrease)/(1024*1024))
	}
}

// CoreWorkersè°ƒæ•´æµ‹è¯•
func runCoreWorkersAdjustmentTest() {
	fmt.Println("\n--- CoreWorkersè°ƒæ•´æµ‹è¯• ---")

	// æµ‹è¯•ä¸åŒç­–ç•¥
	strategies := []struct {
		name     string
		strategy worker_pool.CoreAdjustStrategy
		setup    func(*worker_pool.Pool)
	}{
		{
			name:     "å›ºå®šç­–ç•¥",
			strategy: worker_pool.StrategyFixed,
			setup: func(p *worker_pool.Pool) {
				// å›ºå®šç­–ç•¥é€šè¿‡WithFixedCoreWorkersè®¾ç½®
			},
		},
		{
			name:     "ç™¾åˆ†æ¯”ç­–ç•¥",
			strategy: worker_pool.StrategyPercentage,
			setup:    func(p *worker_pool.Pool) {},
		},
		{
			name:     "æ··åˆç­–ç•¥",
			strategy: worker_pool.StrategyHybrid,
			setup:    func(p *worker_pool.Pool) {},
		},
	}

	for _, s := range strategies {
		fmt.Printf("   æµ‹è¯• %s:\n", s.name)

		var pool *worker_pool.Pool
		if s.name == "å›ºå®šç­–ç•¥" {
			pool = worker_pool.NewPool(10,
				worker_pool.WithMaxWorkers(20),
				worker_pool.WithFixedCoreWorkers(5),
				worker_pool.WithName(s.name),
			)
		} else {
			pool = worker_pool.NewPool(10,
				worker_pool.WithMaxWorkers(20),
				worker_pool.WithCoreAdjustStrategy(s.strategy),
				worker_pool.WithLowLoadThreshold(0.3),
				worker_pool.WithAdjustCheckInterval(500*time.Millisecond),
				worker_pool.WithName(s.name),
			)
		}

		s.setup(pool)

		initialStats := pool.EnhancedStats()
		fmt.Printf("     åˆå§‹çŠ¶æ€: Core=%d, Strategy=%s\n",
			initialStats.CoreWorkers, initialStats.CoreAdjustStrategy)

		// æ‰‹åŠ¨è°ƒæ•´æµ‹è¯•
		pool.SetCoreWorkers(8)
		afterManualStats := pool.EnhancedStats()
		fmt.Printf("     æ‰‹åŠ¨è°ƒæ•´å: Core=%d\n", afterManualStats.CoreWorkers)

		// æäº¤ä¸€äº›ä»»åŠ¡æµ‹è¯•è‡ªåŠ¨è°ƒæ•´
		for i := 0; i < 50; i++ {
			pool.Submit(context.Background(), func(ctx context.Context) (interface{}, error) {
				time.Sleep(10 * time.Millisecond)
				return nil, nil
			})
		}

		// ç­‰å¾…ä»»åŠ¡å®Œæˆå’Œå¯èƒ½çš„è‡ªåŠ¨è°ƒæ•´
		time.Sleep(2 * time.Second)

		finalStats := pool.EnhancedStats()
		fmt.Printf("     æœ€ç»ˆçŠ¶æ€: Core=%d, Active=%d, Completed=%d, History=%d\n",
			finalStats.CoreWorkers, finalStats.ActiveWorkers,
			finalStats.Completed, finalStats.LoadHistoryLength)

		pool.Shutdown()
	}
}

// é”™è¯¯å¤„ç†æµ‹è¯•
func runErrorHandlingTest() {
	fmt.Println("\n--- é”™è¯¯å¤„ç†æµ‹è¯• ---")

	pool := worker_pool.NewPool(3,
		worker_pool.WithMaxWorkers(5),
		worker_pool.WithName("error-test-pool"),
	)
	defer pool.Shutdown()

	var panicCount, errorCount, successCount int64
	var wg sync.WaitGroup

	// æäº¤å„ç§ç±»å‹çš„ä»»åŠ¡
	for i := 0; i < 100; i++ {
		wg.Add(1)

		var taskType int = i % 4

		switch taskType {
		case 0: // æ­£å¸¸ä»»åŠ¡
			pool.Submit(context.Background(), func(ctx context.Context) (interface{}, error) {
				defer wg.Done()
				atomic.AddInt64(&successCount, 1)
				return "success", nil
			})
		case 1: // è¿”å›é”™è¯¯çš„ä»»åŠ¡
			pool.Submit(context.Background(), func(ctx context.Context) (interface{}, error) {
				defer wg.Done()
				atomic.AddInt64(&errorCount, 1)
				return nil, fmt.Errorf("task error %d", i)
			})
		case 2: // Panicä»»åŠ¡ï¼ˆå¸¦æ¢å¤ï¼‰
			pool.Submit(context.Background(), func(ctx context.Context) (interface{}, error) {
				defer wg.Done()
				panic(fmt.Sprintf("task panic %d", i))
			}, worker_pool.WithRecovery(func(r interface{}) {
				atomic.AddInt64(&panicCount, 1)
			}))
		case 3: // å¸¦è¶…æ—¶çš„ä»»åŠ¡
			pool.Submit(context.Background(), func(ctx context.Context) (interface{}, error) {
				defer wg.Done()
				select {
				case <-time.After(50 * time.Millisecond):
					atomic.AddInt64(&successCount, 1)
					return "timeout-success", nil
				case <-ctx.Done():
					atomic.AddInt64(&errorCount, 1)
					return nil, ctx.Err()
				}
			}, worker_pool.WithTimeout(100*time.Millisecond))
		}
	}

	wg.Wait()

	fmt.Printf("âœ… é”™è¯¯å¤„ç†æµ‹è¯•å®Œæˆ:\n")
	fmt.Printf("   - æˆåŠŸä»»åŠ¡: %d\n", successCount)
	fmt.Printf("   - é”™è¯¯ä»»åŠ¡: %d\n", errorCount)
	fmt.Printf("   - Panicä»»åŠ¡: %d\n", panicCount)
	fmt.Printf("   - æ€»è®¡: %d\n", successCount+errorCount+panicCount)

	if successCount+errorCount+panicCount != 100 {
		fmt.Printf("âŒ ä»»åŠ¡æ•°é‡ä¸åŒ¹é…\n")
	}
}

// Contextå–æ¶ˆæµ‹è¯•
func runContextCancellationTest() {
	fmt.Println("\n--- Contextå–æ¶ˆæµ‹è¯• ---")

	pool := worker_pool.NewPool(5,
		worker_pool.WithMaxWorkers(10),
		worker_pool.WithName("context-test-pool"),
	)
	defer pool.Shutdown()

	var cancelledCount, completedCount int64
	var wg sync.WaitGroup

	// æµ‹è¯•ä¸åŒçš„å–æ¶ˆåœºæ™¯
	for i := 0; i < 50; i++ {
		wg.Add(1)

		if i%2 == 0 {
			// å¿«é€Ÿå–æ¶ˆçš„context
			ctx, cancel := context.WithTimeout(context.Background(), 10*time.Millisecond)
			pool.Submit(ctx, func(ctx context.Context) (interface{}, error) {
				defer wg.Done()
				select {
				case <-time.After(50 * time.Millisecond): // ä»»åŠ¡éœ€è¦50ms
					atomic.AddInt64(&completedCount, 1)
					return "completed", nil
				case <-ctx.Done():
					atomic.AddInt64(&cancelledCount, 1)
					return nil, ctx.Err()
				}
			})
			cancel() // ç«‹å³å–æ¶ˆ
		} else {
			// æ­£å¸¸å®Œæˆçš„ä»»åŠ¡
			ctx := context.Background()
			pool.Submit(ctx, func(ctx context.Context) (interface{}, error) {
				defer wg.Done()
				time.Sleep(5 * time.Millisecond)
				atomic.AddInt64(&completedCount, 1)
				return "completed", nil
			})
		}
	}

	wg.Wait()

	fmt.Printf("âœ… Contextå–æ¶ˆæµ‹è¯•å®Œæˆ:\n")
	fmt.Printf("   - å®Œæˆä»»åŠ¡: %d\n", completedCount)
	fmt.Printf("   - å–æ¶ˆä»»åŠ¡: %d\n", cancelledCount)
	fmt.Printf("   - æ€»è®¡: %d\n", completedCount+cancelledCount)

	if completedCount+cancelledCount != 50 {
		fmt.Printf("âŒ ä»»åŠ¡æ•°é‡ä¸åŒ¹é…\n")
	}

	// éªŒè¯å–æ¶ˆçš„ä»»åŠ¡æ•°é‡åˆç†
	if cancelledCount == 0 {
		fmt.Printf("âš ï¸  æ²¡æœ‰ä»»åŠ¡è¢«å–æ¶ˆï¼Œå¯èƒ½contextå–æ¶ˆæœºåˆ¶æœ‰é—®é¢˜\n")
	}
}
*/
