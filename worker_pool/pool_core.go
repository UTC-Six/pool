// worker_pool åŒ…æä¾›äº†ä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„goroutineæ± å®ç°
//
// ğŸ”¥ ç¼©å®¹æœºåˆ¶å®Œæ•´è¯´æ˜ï¼š
//
// 1. ç¼©å®¹å†³ç­–æœºåˆ¶ï¼š
//   - ç”±adjustByPercentage()åŸºäºè´Ÿè½½å†å²ï¼ˆ3å°æ—¶æ•°æ®ï¼‰åˆ†æå†³ç­–
//   - å¦‚æœ80%çš„æ—¶é—´è´Ÿè½½ä½äºé˜ˆå€¼ï¼Œåˆ™è°ƒæ•´coreWorkersä¸ºæ›´å°å€¼
//   - å†³ç­–ç»“æœä½“ç°åœ¨coreWorkerså­—æ®µä¸­ï¼Œè¿™æ˜¯ç¼©å®¹çš„"ç›®æ ‡å€¼"
//
// 2. ç¼©å®¹æ‰§è¡Œæœºåˆ¶ï¼š
//   - workeræ£€æŸ¥ï¼šå½“å‰workeræ•° > coreWorkers ä¸” allowCoreTimeout = true
//   - è¶…å‡ºcoreWorkersçš„workeråœ¨ç©ºé—²keepAliveTimeåè‡ªåŠ¨é€€å‡º
//   - ä¸ä¾èµ–ç¬æ—¶é˜Ÿåˆ—çŠ¶æ€ï¼Œè€Œæ˜¯ç›¸ä¿¡è´Ÿè½½åˆ†æçš„é•¿æœŸè¶‹åŠ¿åˆ¤æ–­
//
// 3. ç¼©å®¹å®ç°ä½ç½®ï¼š
//   - startWorker()æ–¹æ³•ä¸­çš„ç¬¬337è¡Œï¼šp.workers--
//   - è¿™æ˜¯æ•´ä¸ªç³»ç»Ÿä¸­å”¯ä¸€å‡å°‘workeræ•°é‡çš„åœ°æ–¹
//   - æ‰€æœ‰workeréƒ½ä¼šå®šæœŸæ£€æŸ¥coreWorkerså˜åŒ–ï¼Œæ”¯æŒåŠ¨æ€è°ƒæ•´
//
// 4. ç¼©å®¹å®‰å…¨æ€§ï¼š
//   - ä½¿ç”¨åŒé‡æ£€æŸ¥ï¼šè¶…æ—¶å‰æ£€æŸ¥æ¡ä»¶ï¼Œè¶…æ—¶åå†æ¬¡æ£€æŸ¥æ¡ä»¶
//   - åŸå­æ“ä½œï¼šp.workers--åœ¨é”ä¿æŠ¤ä¸‹æ‰§è¡Œ
//   - ä¼˜é›…é€€å‡ºï¼šworker goroutineé€šè¿‡returnæ­£å¸¸ç»“æŸï¼Œæ— å¼ºåˆ¶æ€æ­»
//
// ğŸ›¡ï¸ ä¼˜é›…é€€å‡ºæœºåˆ¶è¯¦è§£ï¼š
//
// 1. ä»€ä¹ˆæ˜¯ä¼˜é›…é€€å‡ºï¼Ÿ
//   - goroutineé€šè¿‡æ­£å¸¸çš„æ§åˆ¶æµç¨‹ï¼ˆreturnï¼‰ç»“æŸ
//   - ä¸æ˜¯è¢«å¤–éƒ¨å¼ºåˆ¶ç»ˆæ­¢ï¼ˆå¦‚å¼ºåˆ¶å…³é—­channelã€panicç­‰ï¼‰
//   - å…è®¸goroutineå®Œæˆæ¸…ç†å·¥ä½œå’Œèµ„æºé‡Šæ”¾
//
// 2. ä¸ºä»€ä¹ˆéœ€è¦ä¼˜é›…é€€å‡ºï¼Ÿ
//   - é¿å…æ•°æ®ä¸¢å¤±ï¼šæ­£åœ¨å¤„ç†çš„ä»»åŠ¡å¯ä»¥å®Œæˆ
//   - é˜²æ­¢èµ„æºæ³„éœ²ï¼šå¯ä»¥æ­£ç¡®é‡Šæ”¾è¿æ¥ã€æ–‡ä»¶å¥æŸ„ç­‰
//   - ä¿è¯ä¸€è‡´æ€§ï¼šé¿å…ä¸­é€”ä¸­æ–­å¯¼è‡´çš„ä¸ä¸€è‡´çŠ¶æ€
//   - æé«˜ç¨³å®šæ€§ï¼šå‡å°‘å› å¼ºåˆ¶ç»ˆæ­¢å¯¼è‡´çš„ç³»ç»Ÿå¼‚å¸¸
//
// 3. æˆ‘ä»¬çš„ä¼˜é›…é€€å‡ºå®ç°ï¼š
//   - ä½¿ç”¨è¶…æ—¶ç­‰å¾…è€Œä¸æ˜¯ç«‹å³é€€å‡º
//   - åŒé‡æ¡ä»¶æ£€æŸ¥ç¡®ä¿é€€å‡ºæ—¶æœºæ­£ç¡®
//   - é€šè¿‡returnè¯­å¥è‡ªç„¶ç»“æŸgoroutineç”Ÿå‘½å‘¨æœŸ
//   - WaitGroupç¡®ä¿æ‰€æœ‰workerå®Œå…¨é€€å‡ºåæ‰å…³é—­æ± 
//
// 5. ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼š
//   - é¿å…å¼ºåˆ¶æ€æ­»workerï¼Œä¿è¯ä»»åŠ¡å®Œæ•´æ€§
//   - åˆ†ç¦»å…³æ³¨ç‚¹ï¼šè°ƒæ•´ç­–ç•¥ vs å®é™…æ‰§è¡Œ
//   - çº¿ç¨‹å®‰å…¨ï¼šæ¯ä¸ªworkerè‡ªå·±å†³å®šæ˜¯å¦é€€å‡º
package worker_pool

import (
	"container/heap"
	"context"
	"fmt"
	"sync"
	"sync/atomic"
	"time"
)

// Pool è¡¨ç¤ºåç¨‹æ± ï¼Œæ”¯æŒè‡ªåŠ¨æ‰©å®¹ã€ä¼˜å…ˆçº§ã€è¶…æ—¶ã€recoveryã€ç»Ÿè®¡ä¿¡æ¯ã€åŠ¨æ€æ ¸å¿ƒworkerè°ƒæ•´
type Pool struct {
	minWorkers  int
	maxWorkers  int
	coreWorkers int // æ ¸å¿ƒworkeræ•°é‡ï¼Œå¯åŠ¨æ€è°ƒæ•´
	mu          sync.Mutex
	workers     int
	taskQueue   taskPriorityQueue
	taskCond    *sync.Cond
	shutdown    bool
	stats       PoolStats
	wg          sync.WaitGroup
	name        string
	logger      func(format string, args ...interface{})

	// åŠ¨æ€è°ƒæ•´ç›¸å…³å­—æ®µ
	allowCoreTimeout  bool          // æ˜¯å¦å…è®¸æ ¸å¿ƒworkerè¶…æ—¶
	keepAliveTime     time.Duration // workerç©ºé—²è¶…æ—¶æ—¶é—´
	lastActivityTime  int64         // æœ€åæ´»åŠ¨æ—¶é—´ï¼ˆçº³ç§’æ—¶é—´æˆ³ï¼‰
	adjustCheckTicker *time.Ticker  // è°ƒæ•´æ£€æŸ¥å®šæ—¶å™¨
	stopAdjustCheck   chan struct{} // åœæ­¢è°ƒæ•´æ£€æŸ¥ä¿¡å·

	// è´Ÿè½½ç»Ÿè®¡
	taskSubmitCount     int64         // ä»»åŠ¡æäº¤è®¡æ•°
	loadHistory         []LoadSample  // è´Ÿè½½å†å²è®°å½•
	loadHistoryMu       sync.RWMutex  // è´Ÿè½½å†å²é”
	adjustCheckInterval time.Duration // è°ƒæ•´æ£€æŸ¥é—´éš”

	// è°ƒæ•´ç­–ç•¥é…ç½®
	coreAdjustStrategy CoreAdjustStrategy // æ ¸å¿ƒworkerè°ƒæ•´ç­–ç•¥
	lowLoadThreshold   float64            // ä½è´Ÿè½½ç™¾åˆ†æ¯”é˜ˆå€¼ (0.0-1.0)
	fixedCoreWorkers   int                // ç”¨æˆ·å›ºå®šè®¾ç½®çš„æ ¸å¿ƒworkeræ•°ï¼Œ0è¡¨ç¤ºä½¿ç”¨åŠ¨æ€è°ƒæ•´
}

// LoadSample è´Ÿè½½é‡‡æ ·æ•°æ®
type LoadSample struct {
	Timestamp     time.Time // é‡‡æ ·æ—¶é—´
	TaskCount     int       // å½“å‰ä»»åŠ¡æ•°é‡
	ActiveWorkers int       // æ´»è·ƒworkeræ•°é‡
	QueueLength   int       // é˜Ÿåˆ—é•¿åº¦
}

// CoreAdjustStrategy æ ¸å¿ƒworkerè°ƒæ•´ç­–ç•¥
type CoreAdjustStrategy int

const (
	// StrategyFixed å›ºå®šç­–ç•¥ï¼šä½¿ç”¨ç”¨æˆ·è®¾å®šçš„å›ºå®šå€¼
	StrategyFixed CoreAdjustStrategy = iota
	// StrategyPercentage ç™¾åˆ†æ¯”ç­–ç•¥ï¼šåŸºäºæœ€è¿‘3å°æ—¶è´Ÿè½½ç™¾åˆ†æ¯”åŠ¨æ€è°ƒæ•´
	StrategyPercentage
	// StrategyHybrid æ··åˆç­–ç•¥ï¼šç»“åˆç™¾åˆ†æ¯”å’Œå›ºå®šå€¼çš„æ™ºèƒ½è°ƒæ•´
	StrategyHybrid
)

// NewPool åˆ›å»ºä¸€ä¸ªåç¨‹æ± ï¼ŒminWorkers å¿…é¡»ï¼Œå…¶ä»–å‚æ•°å¯é€‰
func NewPool(minWorkers int, opts ...PoolOption) *Pool {
	if minWorkers < 1 {
		minWorkers = 1
	}
	p := &Pool{
		minWorkers:  minWorkers,
		maxWorkers:  minWorkers, // é»˜è®¤æœ€å¤§ç­‰äºæœ€å°
		coreWorkers: minWorkers, // é»˜è®¤æ ¸å¿ƒworkerç­‰äºæœ€å°å€¼
		workers:     minWorkers,
		logger:      func(format string, args ...interface{}) {}, // é»˜è®¤ç©ºå®ç°

		// åŠ¨æ€è°ƒæ•´é»˜è®¤é…ç½®
		allowCoreTimeout:    false,
		keepAliveTime:       60 * time.Second,
		adjustCheckInterval: 10 * time.Minute, // é»˜è®¤10åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
		stopAdjustCheck:     make(chan struct{}),
		loadHistory:         make([]LoadSample, 0, 180), // 3å°æ—¶ï¼Œæ¯åˆ†é’Ÿä¸€ä¸ªæ ·æœ¬

		// è°ƒæ•´ç­–ç•¥é»˜è®¤é…ç½®
		coreAdjustStrategy: StrategyPercentage, // é»˜è®¤ä½¿ç”¨ç™¾åˆ†æ¯”ç­–ç•¥
		lowLoadThreshold:   0.3,                // é»˜è®¤30%é˜ˆå€¼
		fixedCoreWorkers:   0,                  // é»˜è®¤ä½¿ç”¨åŠ¨æ€è°ƒæ•´
	}
	for _, opt := range opts {
		opt(p)
	}
	p.taskCond = sync.NewCond(&p.mu)
	heap.Init(&p.taskQueue)
	for i := 0; i < minWorkers; i++ {
		p.startWorker()
	}

	// å¯åŠ¨åŠ¨æ€è°ƒæ•´ç›‘æ§
	p.startLoadMonitoring()

	if p.logger != nil && p.name != "" {
		p.logger("Pool %s created with min=%d, max=%d, core=%d", p.name, p.minWorkers, p.maxWorkers, p.coreWorkers)
	}
	return p
}

// PoolOption ç”¨äºé…ç½® Pool çš„å¯é€‰å‚æ•°
type PoolOption func(*Pool)

// WithMaxWorkers è®¾ç½®æœ€å¤§ worker æ•°
func WithMaxWorkers(max int) PoolOption {
	return func(p *Pool) {
		if max < p.minWorkers {
			max = p.minWorkers
		}
		p.maxWorkers = max
	}
}

// WithName è®¾ç½®æ± çš„åå­—
func WithName(name string) PoolOption {
	return func(p *Pool) {
		p.name = name
	}
}

// WithLogger è®¾ç½®æ± çš„æ—¥å¿—å‡½æ•°
func WithLogger(logger func(format string, args ...interface{})) PoolOption {
	return func(p *Pool) {
		p.logger = logger
	}
}

// WithAllowCoreTimeout å…è®¸æ ¸å¿ƒworkerè¶…æ—¶é€€å‡º
func WithAllowCoreTimeout(allow bool) PoolOption {
	return func(p *Pool) {
		p.allowCoreTimeout = allow
	}
}

// WithKeepAliveTime è®¾ç½®workerç©ºé—²è¶…æ—¶æ—¶é—´
func WithKeepAliveTime(duration time.Duration) PoolOption {
	return func(p *Pool) {
		p.keepAliveTime = duration
	}
}

// WithAdjustCheckInterval è®¾ç½®è°ƒæ•´æ£€æŸ¥é—´éš”
func WithAdjustCheckInterval(interval time.Duration) PoolOption {
	return func(p *Pool) {
		p.adjustCheckInterval = interval
	}
}

// WithCoreAdjustStrategy è®¾ç½®æ ¸å¿ƒworkerè°ƒæ•´ç­–ç•¥
func WithCoreAdjustStrategy(strategy CoreAdjustStrategy) PoolOption {
	return func(p *Pool) {
		p.coreAdjustStrategy = strategy
	}
}

// WithLowLoadThreshold è®¾ç½®ä½è´Ÿè½½ç™¾åˆ†æ¯”é˜ˆå€¼ (0.0-1.0)
func WithLowLoadThreshold(threshold float64) PoolOption {
	return func(p *Pool) {
		if threshold < 0.0 {
			threshold = 0.0
		} else if threshold > 1.0 {
			threshold = 1.0
		}
		p.lowLoadThreshold = threshold
	}
}

// WithFixedCoreWorkers è®¾ç½®å›ºå®šçš„æ ¸å¿ƒworkeræ•°é‡ï¼Œ0è¡¨ç¤ºä½¿ç”¨åŠ¨æ€è°ƒæ•´
func WithFixedCoreWorkers(core int) PoolOption {
	return func(p *Pool) {
		if core < 0 {
			core = 0
		}
		p.fixedCoreWorkers = core
		if core > 0 {
			p.coreAdjustStrategy = StrategyFixed
			p.coreWorkers = core
		}
	}
}

// Submit æäº¤ä¸€ä¸ªä»»åŠ¡åˆ°æ± ä¸­ï¼Œæ”¯æŒå¯é€‰å‚æ•°
// - å¹¶å‘å®‰å…¨ï¼šå…¨ç¨‹æŒæœ‰é”ï¼Œä¿è¯ä»»åŠ¡é˜Ÿåˆ—å’Œæ± çŠ¶æ€ä¸€è‡´æ€§
// - ctx: æ¨èä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ï¼Œä¾¿äºä»»åŠ¡å–æ¶ˆ/è¶…æ—¶/ç»Ÿä¸€ç®¡ç†
// - Option: æ¨èç”¨ Option ä¼ é€’å¯é€‰å‚æ•°ï¼Œçµæ´»æ‰©å±•
// - é»˜è®¤è¶…æ—¶æ—¶é—´ï¼š3ç§’ï¼Œä¸šåŠ¡å¯ç”¨ WithTimeout è¦†ç›–
func (p *Pool) Submit(ctx context.Context, taskFunc func(ctx context.Context) (interface{}, error), opts ...TaskOption) error {
	if taskFunc == nil {
		return fmt.Errorf("taskFunc cannot be nil")
	}

	// è®°å½•æ´»åŠ¨æ—¶é—´å’Œä»»åŠ¡è®¡æ•°
	atomic.StoreInt64(&p.lastActivityTime, time.Now().UnixNano())
	atomic.AddInt64(&p.taskSubmitCount, 1)

	p.mu.Lock()
	defer p.mu.Unlock()
	if p.shutdown {
		return ErrPoolClosed
	}
	task := &Task{
		Priority: PriorityNormal,     // é»˜è®¤
		Timeout:  DefaultTaskTimeout, // é»˜è®¤è¶…æ—¶3ç§’ï¼Œä¸šåŠ¡å¯è¦†ç›–
		TaskFunc: taskFunc,
		Ctx:      ctx, // æ–°å¢
	}
	for _, opt := range opts {
		opt(task)
	}
	heap.Push(&p.taskQueue, task)
	p.taskCond.Signal()
	p.autoScale()
	return nil
}

// SubmitWithResult æäº¤å¸¦è¿”å›å€¼çš„ä»»åŠ¡ï¼Œæ”¯æŒå¯é€‰å‚æ•°
// - å¹¶å‘å®‰å…¨ï¼šå…¨ç¨‹æŒæœ‰é”ï¼Œä¿è¯ä»»åŠ¡é˜Ÿåˆ—å’Œæ± çŠ¶æ€ä¸€è‡´æ€§
// - ctx: æ¨èä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ï¼Œä¾¿äºä»»åŠ¡å–æ¶ˆ/è¶…æ—¶/ç»Ÿä¸€ç®¡ç†
// - Option: æ¨èç”¨ Option ä¼ é€’å¯é€‰å‚æ•°ï¼Œçµæ´»æ‰©å±•
// - é»˜è®¤è¶…æ—¶æ—¶é—´ï¼š3ç§’ï¼Œä¸šåŠ¡å¯ç”¨ WithTimeout è¦†ç›–
func (p *Pool) SubmitWithResult(ctx context.Context, taskFunc func(ctx context.Context) (interface{}, error), opts ...TaskOption) (<-chan TaskResult, error) {
	if taskFunc == nil {
		return nil, fmt.Errorf("taskFunc cannot be nil")
	}

	// è®°å½•æ´»åŠ¨æ—¶é—´å’Œä»»åŠ¡è®¡æ•°
	atomic.StoreInt64(&p.lastActivityTime, time.Now().UnixNano())
	atomic.AddInt64(&p.taskSubmitCount, 1)

	p.mu.Lock()
	defer p.mu.Unlock()
	if p.shutdown {
		return nil, ErrPoolClosed
	}
	resultChan := make(chan TaskResult, 1)
	task := &Task{
		Priority:   PriorityNormal,
		Timeout:    DefaultTaskTimeout, // é»˜è®¤è¶…æ—¶3ç§’ï¼Œä¸šåŠ¡å¯è¦†ç›–
		TaskFunc:   taskFunc,
		ResultChan: resultChan,
		Ctx:        ctx, // æ–°å¢
	}
	for _, opt := range opts {
		opt(task)
	}
	heap.Push(&p.taskQueue, task)
	p.taskCond.Signal()
	p.autoScale()
	return resultChan, nil
}

// startWorker å¯åŠ¨ä¸€ä¸ªworker goroutine
// ğŸ”¥ ç¼©å®¹æœºåˆ¶è¯¦è§£ï¼š
// 1. workeråœ¨ç©ºé—²æ—¶ä¼šæ£€æŸ¥æ˜¯å¦å¯ä»¥é€€å‡ºï¼ˆåŸºäºkeepAliveTimeè¶…æ—¶ï¼‰
// 2. é€€å‡ºæ¡ä»¶ï¼šallowCoreTimeout=true ä¸” å½“å‰workeræ•° > coreWorkers ä¸” ç©ºé—²è¶…è¿‡keepAliveTime
// 3. é€šè¿‡p.workers--å®ç°workeræ•°é‡çš„å‡å°‘ï¼Œè¿™æ˜¯å”¯ä¸€çš„ç¼©å®¹å®ç°ä½ç½®
func (p *Pool) startWorker() {
	p.wg.Add(1)
	go func() {
		defer p.wg.Done()

		for {
			p.mu.Lock()

			// === ğŸ¯ ç¼©å®¹é€»è¾‘æ ¸å¿ƒå®ç° ===
			// ğŸ”¥ æ­£ç¡®çš„ç¼©å®¹é€»è¾‘ï¼š
			// 1. ç¼©å®¹å†³ç­–ç”±adjustByPercentage()åŸºäºè´Ÿè½½å†å²åšå‡ºï¼Œä½“ç°åœ¨coreWorkerså€¼ä¸­
			// 2. workeråªéœ€è¦æ£€æŸ¥ï¼šå½“å‰workeræ•° > coreWorkers ä¸” å…è®¸è¶…æ—¶
			// 3. ä¸åº”è¯¥ä¾èµ–ç¬æ—¶çš„é˜Ÿåˆ—çŠ¶æ€ï¼Œè€Œåº”è¯¥ç›¸ä¿¡è´Ÿè½½åˆ†æçš„ç»“æœ

			// æ£€æŸ¥æ˜¯å¦å¯ä»¥è¶…æ—¶é€€å‡ºï¼ˆåŸºäºè´Ÿè½½åˆ†æç»“æœï¼‰
			canTimeout := p.allowCoreTimeout && p.workers > p.coreWorkers

			if len(p.taskQueue) == 0 && !p.shutdown {
				if canTimeout {
					// ğŸ¯ è¶…å‡ºCoreWorkersçš„workerï¼šä½¿ç”¨keepAliveTimeè¶…æ—¶ç­‰å¾…
					// è¿™äº›workeræ˜¯"å¤šä½™"çš„ï¼Œåº”è¯¥åœ¨ç©ºé—²æ—¶é€€å‡ºä»¥èŠ‚çœèµ„æº
					p.mu.Unlock()

					ctx, cancel := context.WithTimeout(context.Background(), p.keepAliveTime)
					done := make(chan struct{})

					go func() {
						p.mu.Lock()
						for len(p.taskQueue) == 0 && !p.shutdown {
							p.taskCond.Wait()
						}
						p.mu.Unlock()
						close(done)
					}()

					select {
					case <-done:
						cancel()
						// æœ‰ä»»åŠ¡åˆ°è¾¾ï¼Œç»§ç»­å¤„ç†
					case <-ctx.Done():
						cancel()
						// ğŸ›¡ï¸ ä¼˜é›…é€€å‡ºï¼šåŸºäºè´Ÿè½½åˆ†æçš„æ™ºèƒ½ç¼©å®¹
						p.mu.Lock()
						if p.workers > p.coreWorkers { // å†æ¬¡ç¡®è®¤ä»ç„¶è¶…å‡ºæ ¸å¿ƒæ•°
							p.workers-- // ğŸ”¥ ç¼©å®¹å®ç°ï¼šå‡å°‘workerè®¡æ•°
							if p.logger != nil {
								p.logger("Worker shrunk due to low load, workers: %d -> %d (coreWorkers: %d)",
									p.workers+1, p.workers, p.coreWorkers)
							}
							p.mu.Unlock()

							// ğŸ¯ å…³é”®ï¼šé€šè¿‡returnä¼˜é›…é€€å‡ºgoroutine
							// è¿™é‡Œä¸æ˜¯è¢«å¼ºåˆ¶æ€æ­»ï¼Œè€Œæ˜¯workerè‡ªä¸»å†³å®šé€€å‡º
							// æ‰€æœ‰æ¸…ç†å·¥ä½œéƒ½ä¼šè¢«deferè¯­å¥æ­£ç¡®æ‰§è¡Œ
							// WaitGroupä¼šè¢«æ­£ç¡®é€’å‡ï¼Œç¡®ä¿Shutdown()èƒ½æ­£å¸¸å®Œæˆ
							return // ä¼˜é›…é€€å‡ºï¼šgoroutineè‡ªç„¶ç»“æŸç”Ÿå‘½å‘¨æœŸ
						}
						p.mu.Unlock()
					}
					continue
				} else {
					// ğŸ¯ CoreWorkersèŒƒå›´å†…çš„workerï¼šå®šæœŸæ£€æŸ¥æ˜¯å¦å˜æˆ"å¤šä½™"worker
					// å½“coreWorkersåŠ¨æ€è°ƒæ•´åï¼ŒåŸæœ¬çš„æ ¸å¿ƒworkerå¯èƒ½å˜æˆå¤šä½™worker
					p.mu.Unlock()

					// ä½¿ç”¨è¾ƒçŸ­è¶…æ—¶å®šæœŸæ£€æŸ¥coreWorkerså˜åŒ–
					ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
					done := make(chan struct{})

					go func() {
						p.mu.Lock()
						for len(p.taskQueue) == 0 && !p.shutdown {
							p.taskCond.Wait()
						}
						p.mu.Unlock()
						close(done)
					}()

					select {
					case <-done:
						cancel()
						// æœ‰ä»»åŠ¡åˆ°è¾¾ï¼Œç»§ç»­å¤„ç†
					case <-ctx.Done():
						cancel()
						// è¶…æ—¶ï¼Œé‡æ–°æ£€æŸ¥æ¡ä»¶ï¼ˆä¸‹ä¸€æ¬¡å¾ªç¯ä¼šé‡æ–°è¯„ä¼°canTimeoutï¼‰
					}
					continue
				}
			}

			if p.shutdown && len(p.taskQueue) == 0 {
				p.mu.Unlock()
				return
			}

			task := heap.Pop(&p.taskQueue).(*Task)
			p.stats.ActiveWorkers++
			p.stats.QueuedTasks = len(p.taskQueue)
			p.mu.Unlock()

			// å¤„ç†ä»»åŠ¡ï¼Œctx ç”± Task.Ctx ä¼ é€’
			p.handleTask(task.Ctx, task)

			p.mu.Lock()
			p.stats.ActiveWorkers--
			p.stats.Completed++
			p.stats.QueuedTasks = len(p.taskQueue)
			p.mu.Unlock()
		}
	}()
}

// handleTask å¤„ç†å•ä¸ªä»»åŠ¡ï¼Œctx ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°
func (p *Pool) handleTask(ctx context.Context, task *Task) {
	if ctx == nil {
		ctx = context.Background()
	}
	if task.Timeout > 0 {
		var cancel context.CancelFunc
		ctx, cancel = context.WithTimeout(ctx, task.Timeout)
		defer cancel()
	}
	if task.Before != nil {
		task.Before()
	}
	if task.LogFn != nil {
		task.LogFn("[Task] start tag=%s", task.Tag)
	}
	defer func() {
		if r := recover(); r != nil && task.Recovery != nil {
			task.Recovery(r)
		}
		if task.After != nil {
			task.After()
		}
		if task.LogFn != nil {
			task.LogFn("[Task] end tag=%s", task.Tag)
		}
	}()
	result, err := task.TaskFunc(ctx)
	if task.ResultChan != nil {
		task.ResultChan <- TaskResult{Result: result, Err: err}
		close(task.ResultChan)
	}
}

// autoScale æ ¹æ®ä»»åŠ¡é˜Ÿåˆ—é•¿åº¦è‡ªåŠ¨æ‰©å®¹worker
// ğŸ”’ çº¿ç¨‹å®‰å…¨æ€§åˆ†æï¼š
// 1. æ­¤æ–¹æ³•åœ¨Submit/SubmitWithResultä¸­è°ƒç”¨ï¼Œè°ƒç”¨æ—¶å·²æŒæœ‰p.mué”
// 2. è®¿é—®çš„å­—æ®µ(p.workers, p.maxWorkers, len(p.taskQueue))éƒ½åœ¨é”ä¿æŠ¤ä¸‹
// 3. p.startWorker()å†…éƒ¨ä¼šå¯åŠ¨æ–°goroutineï¼Œä½†ä¸éœ€è¦é¢å¤–é”ä¿æŠ¤
// 4. å› æ­¤è¿™ä¸ªæ–¹æ³•æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œä¸éœ€è¦é¢å¤–åŠ é”
func (p *Pool) autoScale() {
	if p.workers >= p.maxWorkers {
		return
	}
	if len(p.taskQueue) > p.workers {
		p.workers++
		p.startWorker()
	}
}

// Stats è¿”å›å½“å‰æ± çš„ç»Ÿè®¡ä¿¡æ¯
func (p *Pool) Stats() PoolStats {
	p.mu.Lock()
	defer p.mu.Unlock()
	return PoolStats{
		ActiveWorkers: p.stats.ActiveWorkers,
		QueuedTasks:   len(p.taskQueue),
		Completed:     p.stats.Completed,
		MinWorkers:    p.minWorkers,
		MaxWorkers:    p.maxWorkers,
	}
}

// Shutdown ä¼˜é›…å…³é—­æ± ï¼Œç­‰å¾…æ‰€æœ‰ä»»åŠ¡å’Œworkerå®Œæˆ
// ğŸ›¡ï¸ ä¼˜é›…å…³é—­æœºåˆ¶ï¼š
// 1. åœæ­¢è´Ÿè½½ç›‘æ§goroutine
// 2. è®¾ç½®shutdownæ ‡å¿—ï¼Œé˜»æ­¢æ–°ä»»åŠ¡æäº¤
// 3. å¹¿æ’­å”¤é†’æ‰€æœ‰ç­‰å¾…çš„worker
// 4. ç­‰å¾…æ‰€æœ‰workerä¼˜é›…é€€å‡ºï¼ˆé€šè¿‡WaitGroupï¼‰
// 5. ç¡®ä¿æ²¡æœ‰goroutineæ³„éœ²
func (p *Pool) Shutdown() {
	// ç¬¬ä¸€æ­¥ï¼šåœæ­¢è´Ÿè½½ç›‘æ§goroutine
	p.mu.Lock()
	if p.stopAdjustCheck != nil {
		select {
		case <-p.stopAdjustCheck:
			// channel already closed
		default:
			close(p.stopAdjustCheck) // ä¼˜é›…åœæ­¢ç›‘æ§goroutine
		}
	}

	// ç¬¬äºŒæ­¥ï¼šè®¾ç½®å…³é—­æ ‡å¿—ï¼Œé˜»æ­¢æ–°ä»»åŠ¡æäº¤
	p.shutdown = true

	// ç¬¬ä¸‰æ­¥ï¼šå”¤é†’æ‰€æœ‰ç­‰å¾…çš„workerï¼Œè®©å®ƒä»¬æ£€æŸ¥shutdownæ ‡å¿—
	p.taskCond.Broadcast() // å¹¿æ’­ä¿¡å·ï¼Œå”¤é†’æ‰€æœ‰Wait()ä¸­çš„worker
	p.mu.Unlock()

	// ç¬¬å››æ­¥ï¼šç­‰å¾…æ‰€æœ‰workerä¼˜é›…é€€å‡º
	// ğŸ¯ å…³é”®ï¼šè¿™é‡Œä¼šé˜»å¡ç›´åˆ°æ‰€æœ‰workeré€šè¿‡returnä¼˜é›…é€€å‡º
	// æ¯ä¸ªworkeråœ¨é€€å‡ºæ—¶ä¼šè°ƒç”¨wg.Done()ï¼Œå½“æ‰€æœ‰workeréƒ½é€€å‡ºåWait()æ‰ä¼šè¿”å›
	p.wg.Wait() // ç¡®ä¿æ‰€æœ‰workeréƒ½å·²ä¼˜é›…é€€å‡ºï¼Œæ— goroutineæ³„éœ²

	// åˆ°è¿™é‡Œï¼Œå¯ä»¥ä¿è¯ï¼š
	// - æ‰€æœ‰æ­£åœ¨å¤„ç†çš„ä»»åŠ¡éƒ½å·²å®Œæˆ
	// - æ‰€æœ‰worker goroutineéƒ½å·²ä¼˜é›…é€€å‡º
	// - æ²¡æœ‰goroutineæ³„éœ²
	// - æ‰€æœ‰èµ„æºéƒ½å·²æ­£ç¡®æ¸…ç†
}

// SetMinWorkers åŠ¨æ€è®¾ç½®æœ€å° worker æ•°
func (p *Pool) SetMinWorkers(min int) {
	p.mu.Lock()
	defer p.mu.Unlock()
	if min < 1 {
		min = 1
	}
	if min > p.maxWorkers {
		// è‡ªåŠ¨æ‰©å®¹ maxWorkers ä»¥é€‚é…æ›´å¤§çš„ minWorkers
		p.maxWorkers = min
	}
	p.minWorkers = min
	// å¢åŠ  worker
	for p.workers < p.minWorkers {
		p.workers++
		p.startWorker()
	}
}

// SetMaxWorkers åŠ¨æ€è®¾ç½®æœ€å¤§ worker æ•°
func (p *Pool) SetMaxWorkers(max int) {
	p.mu.Lock()
	defer p.mu.Unlock()
	if max < p.minWorkers {
		// è‡ªåŠ¨æ”¶ç¼© minWorkers ä»¥é€‚é…æ›´å°çš„ maxWorkers
		p.minWorkers = max
	}
	p.maxWorkers = max
	// å¦‚æœå½“å‰ worker è¶…è¿‡ maxWorkersï¼Œworker ä¼šåœ¨ç©ºé—²æ—¶è‡ªç„¶é€€å‡º
}

// Restart é‡æ–°å¯åŠ¨æ± ï¼ˆå…³é—­åé‡å¯ï¼‰
func (p *Pool) Restart() {
	p.mu.Lock()
	if !p.shutdown {
		p.mu.Unlock()
		return
	}
	p.shutdown = false
	p.stats = PoolStats{}
	p.workers = p.minWorkers
	heap.Init(&p.taskQueue)
	p.taskCond = sync.NewCond(&p.mu)
	for i := 0; i < p.minWorkers; i++ {
		p.startWorker()
	}
	p.mu.Unlock()
}

// startLoadMonitoring å¯åŠ¨è´Ÿè½½ç›‘æ§å’ŒåŠ¨æ€è°ƒæ•´
func (p *Pool) startLoadMonitoring() {
	if p.adjustCheckInterval <= 0 {
		return // ä¸å¯åŠ¨ç›‘æ§
	}

	p.adjustCheckTicker = time.NewTicker(p.adjustCheckInterval)

	go func() {
		defer p.adjustCheckTicker.Stop()
		for {
			select {
			case <-p.adjustCheckTicker.C:
				p.collectLoadSample()
				p.adjustCoreWorkers()
			case <-p.stopAdjustCheck:
				return
			}
		}
	}()
}

// collectLoadSample æ”¶é›†è´Ÿè½½æ ·æœ¬
func (p *Pool) collectLoadSample() {
	p.mu.Lock()
	sample := LoadSample{
		Timestamp:     time.Now(),
		TaskCount:     int(atomic.LoadInt64(&p.taskSubmitCount)),
		ActiveWorkers: p.stats.ActiveWorkers,
		QueueLength:   len(p.taskQueue),
	}
	p.mu.Unlock()

	p.loadHistoryMu.Lock()
	defer p.loadHistoryMu.Unlock()

	// æ·»åŠ æ–°æ ·æœ¬
	p.loadHistory = append(p.loadHistory, sample)

	// ä¿æŒæœ€è¿‘3å°æ—¶çš„æ•°æ®ï¼ˆ180ä¸ªæ ·æœ¬ï¼Œæ¯åˆ†é’Ÿä¸€ä¸ªï¼‰
	maxSamples := int(3 * time.Hour / p.adjustCheckInterval)
	if len(p.loadHistory) > maxSamples {
		p.loadHistory = p.loadHistory[len(p.loadHistory)-maxSamples:]
	}
}

// adjustCoreWorkers æ ¹æ®ç­–ç•¥è°ƒæ•´æ ¸å¿ƒworkeræ•°é‡
func (p *Pool) adjustCoreWorkers() {
	if p.coreAdjustStrategy == StrategyFixed {
		return // å›ºå®šç­–ç•¥ä¸è°ƒæ•´
	}

	p.loadHistoryMu.RLock()
	historyLen := len(p.loadHistory)
	p.loadHistoryMu.RUnlock()

	if historyLen < 10 { // è‡³å°‘éœ€è¦10ä¸ªæ ·æœ¬
		return
	}

	switch p.coreAdjustStrategy {
	case StrategyPercentage:
		p.adjustByPercentage()
	case StrategyHybrid:
		p.adjustByHybrid()
	}
}

// adjustByPercentage åŸºäºè´Ÿè½½å†å²ç™¾åˆ†æ¯”è°ƒæ•´æ ¸å¿ƒworkeræ•°é‡
// ğŸ”’ çº¿ç¨‹å®‰å…¨æ€§åˆ†æï¼š
// 1. ä½¿ç”¨loadHistoryMu.RLock()ä¿æŠ¤è´Ÿè½½å†å²æ•°æ®çš„è¯»å–
// 2. è®¿é—®p.minWorkers, p.lowLoadThresholdç­‰é…ç½®å­—æ®µæ—¶æ— é”ï¼ˆè¿™äº›æ˜¯åªè¯»é…ç½®ï¼‰
// 3. ä¿®æ”¹p.coreWorkersæ—¶ä½¿ç”¨p.mu.Lock()ä¿æŠ¤
// 4. é‡‡ç”¨åŒé‡é”è®¾è®¡ï¼šè¯»é”ä¿æŠ¤æ•°æ®è¯»å–ï¼Œå†™é”ä¿æŠ¤çŠ¶æ€ä¿®æ”¹
// 5. çº¿ç¨‹å®‰å…¨ï¼Œè®¾è®¡åˆç†
func (p *Pool) adjustByPercentage() {
	p.loadHistoryMu.RLock()
	defer p.loadHistoryMu.RUnlock()

	// è®¡ç®—æœ€è¿‘3å°æ—¶å†…ä½è´Ÿè½½çš„ç™¾åˆ†æ¯”
	lowLoadCount := 0
	totalSamples := len(p.loadHistory)

	for _, sample := range p.loadHistory {
		// å¦‚æœæ´»è·ƒworkeræ•°é‡ä½äºminWorkersçš„é˜ˆå€¼ç™¾åˆ†æ¯”ï¼Œè®¤ä¸ºæ˜¯ä½è´Ÿè½½
		threshold := float64(p.minWorkers) * p.lowLoadThreshold
		if float64(sample.ActiveWorkers) < threshold && sample.QueueLength == 0 {
			lowLoadCount++
		}
	}

	lowLoadRatio := float64(lowLoadCount) / float64(totalSamples)

	// å¦‚æœä½è´Ÿè½½æ¯”ä¾‹è¶…è¿‡80%ï¼Œè€ƒè™‘å‡å°‘æ ¸å¿ƒworkeræ•°é‡
	if lowLoadRatio > 0.8 {
		suggestedCore := int(float64(p.minWorkers) * p.lowLoadThreshold)
		if suggestedCore < 1 {
			suggestedCore = 1
		}

		p.mu.Lock()
		if suggestedCore < p.coreWorkers {
			oldCore := p.coreWorkers
			p.coreWorkers = suggestedCore
			if p.logger != nil {
				p.logger("Core workers adjusted from %d to %d (low load ratio: %.2f%%)",
					oldCore, p.coreWorkers, lowLoadRatio*100)
			}
		}
		p.mu.Unlock()
	} else if lowLoadRatio < 0.3 {
		// å¦‚æœä½è´Ÿè½½æ¯”ä¾‹ä½äº30%ï¼Œè€ƒè™‘æ¢å¤æ ¸å¿ƒworkeræ•°é‡åˆ°minWorkers
		// æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯è°ƒæ•´coreWorkersæ•°é‡ï¼Œå®é™…çš„workeræ‰©å®¹ç”±autoScale()å¤„ç†
		p.mu.Lock()
		if p.coreWorkers < p.minWorkers {
			oldCore := p.coreWorkers
			p.coreWorkers = p.minWorkers
			if p.logger != nil {
				p.logger("Core workers restored from %d to %d (load increased)",
					oldCore, p.coreWorkers)
			}
		}
		p.mu.Unlock()
	}
}

// adjustByHybrid æ··åˆç­–ç•¥è°ƒæ•´
func (p *Pool) adjustByHybrid() {
	// å…ˆæ‰§è¡Œç™¾åˆ†æ¯”ç­–ç•¥
	p.adjustByPercentage()

	// ç„¶åè€ƒè™‘ç”¨æˆ·è®¾ç½®çš„å›ºå®šå€¼
	if p.fixedCoreWorkers > 0 {
		p.mu.Lock()
		if p.coreWorkers != p.fixedCoreWorkers {
			oldCore := p.coreWorkers
			p.coreWorkers = p.fixedCoreWorkers
			if p.logger != nil {
				p.logger("Core workers set to fixed value from %d to %d",
					oldCore, p.coreWorkers)
			}
		}
		p.mu.Unlock()
	}
}

// SetCoreWorkers æ‰‹åŠ¨è®¾ç½®æ ¸å¿ƒworkeræ•°é‡
func (p *Pool) SetCoreWorkers(core int) {
	p.mu.Lock()
	defer p.mu.Unlock()

	if core < 1 {
		core = 1
	}
	if core > p.maxWorkers {
		core = p.maxWorkers
	}

	oldCore := p.coreWorkers
	p.coreWorkers = core

	if p.logger != nil && oldCore != core {
		p.logger("Core workers manually set from %d to %d", oldCore, core)
	}
}

// GetCoreWorkers è·å–å½“å‰æ ¸å¿ƒworkeræ•°é‡
func (p *Pool) GetCoreWorkers() int {
	p.mu.Lock()
	defer p.mu.Unlock()
	return p.coreWorkers
}

// GetLoadHistory è·å–è´Ÿè½½å†å²ï¼ˆç”¨äºç›‘æ§å’Œè°ƒè¯•ï¼‰
func (p *Pool) GetLoadHistory() []LoadSample {
	p.loadHistoryMu.RLock()
	defer p.loadHistoryMu.RUnlock()

	// è¿”å›å‰¯æœ¬é¿å…å¹¶å‘é—®é¢˜
	history := make([]LoadSample, len(p.loadHistory))
	copy(history, p.loadHistory)
	return history
}

// DetailedStats è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…å«æ‰€æœ‰ç›‘æ§æŒ‡æ ‡
type DetailedStats struct {
	PoolStats
	CoreWorkers        int       `json:"core_workers"`
	LastActivityTime   time.Time `json:"last_activity_time"`
	TaskSubmitCount    int64     `json:"task_submit_count"`
	AllowCoreTimeout   bool      `json:"allow_core_timeout"`
	CoreAdjustStrategy string    `json:"core_adjust_strategy"`
	LowLoadThreshold   float64   `json:"low_load_threshold"`
	LoadHistoryLength  int       `json:"load_history_length"`
}

// DetailedStats è·å–è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
func (p *Pool) DetailedStats() DetailedStats {
	p.mu.Lock()
	// ç›´æ¥æ„å»ºPoolStatsï¼Œé¿å…è°ƒç”¨p.Stats()å¯¼è‡´æ­»é”
	baseStats := PoolStats{
		ActiveWorkers: p.stats.ActiveWorkers,
		QueuedTasks:   len(p.taskQueue),
		Completed:     p.stats.Completed,
		MinWorkers:    p.minWorkers,
		MaxWorkers:    p.maxWorkers,
	}
	coreWorkers := p.coreWorkers
	allowCoreTimeout := p.allowCoreTimeout
	strategy := p.coreAdjustStrategy
	threshold := p.lowLoadThreshold
	p.mu.Unlock()

	p.loadHistoryMu.RLock()
	historyLen := len(p.loadHistory)
	p.loadHistoryMu.RUnlock()

	strategyStr := "Unknown"
	switch strategy {
	case StrategyFixed:
		strategyStr = "Fixed"
	case StrategyPercentage:
		strategyStr = "Percentage"
	case StrategyHybrid:
		strategyStr = "Hybrid"
	}

	return DetailedStats{
		PoolStats:          baseStats,
		CoreWorkers:        coreWorkers,
		LastActivityTime:   time.Unix(0, atomic.LoadInt64(&p.lastActivityTime)),
		TaskSubmitCount:    atomic.LoadInt64(&p.taskSubmitCount),
		AllowCoreTimeout:   allowCoreTimeout,
		CoreAdjustStrategy: strategyStr,
		LowLoadThreshold:   threshold,
		LoadHistoryLength:  historyLen,
	}
}
