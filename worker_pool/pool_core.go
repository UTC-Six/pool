package worker_pool

import (
	"container/heap"
	"context"
	"fmt"
	"sync"
	"sync/atomic"
	"time"
)

// Pool è¡¨ç¤ºåç¨‹æ± ï¼Œè¿™æ˜¯æ•´ä¸ªworker_poolçš„æ ¸å¿ƒç»“æ„ä½“
// ğŸ¯ æ ¸å¿ƒè®¾è®¡ç†å¿µï¼š
// 1. é¢„åˆ†é…worker goroutineï¼Œé¿å…é¢‘ç¹åˆ›å»ºé”€æ¯çš„å¼€é”€
// 2. ä½¿ç”¨ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼Œæ”¯æŒä»»åŠ¡ä¼˜å…ˆçº§è°ƒåº¦
// 3. åŠ¨æ€æ‰©ç¼©å®¹ï¼Œæ ¹æ®è´Ÿè½½è‡ªåŠ¨è°ƒæ•´workeræ•°é‡
// 4. æ™ºèƒ½èµ„æºç®¡ç†ï¼Œæ”¯æŒCoreWorkersåŠ¨æ€è°ƒæ•´èŠ‚çœèµ„æº
type Pool struct {
	// === åŸºç¡€é…ç½® ===
	minWorkers  int // æœ€å°workeræ•°é‡ï¼Œæ± å¯åŠ¨æ—¶åˆ›å»ºçš„åŸºç¡€workeræ•°
	maxWorkers  int // æœ€å¤§workeræ•°é‡ï¼Œé«˜è´Ÿè½½æ—¶çš„æ‰©å®¹ä¸Šé™
	coreWorkers int // ğŸ”¥ æ ¸å¿ƒworkeræ•°é‡ï¼Œå¯åŠ¨æ€è°ƒæ•´çš„å¸¸é©»workeræ•°ï¼ˆä¸šç•Œåˆ›æ–°ç‰¹æ€§ï¼‰

	// === å¹¶å‘æ§åˆ¶ ===
	mu       sync.Mutex     // ä¸»é”ï¼Œä¿æŠ¤æ± çš„æ ¸å¿ƒçŠ¶æ€
	workers  int            // å½“å‰å®é™…workeræ•°é‡
	taskCond *sync.Cond     // æ¡ä»¶å˜é‡ï¼Œç”¨äºworkerç­‰å¾…ä»»åŠ¡å’Œå”¤é†’
	shutdown bool           // å…³é—­æ ‡å¿—ï¼Œæ ‡è®°æ± æ˜¯å¦æ­£åœ¨å…³é—­
	wg       sync.WaitGroup // ç­‰å¾…ç»„ï¼Œç¡®ä¿æ‰€æœ‰workerä¼˜é›…é€€å‡º

	// === ä»»åŠ¡é˜Ÿåˆ— ===
	taskQueue taskPriorityQueue // ğŸ¯ ä¼˜å…ˆçº§ä»»åŠ¡é˜Ÿåˆ—ï¼ŒåŸºäºheapå®ç°ï¼Œæ”¯æŒé«˜ä¼˜å…ˆçº§ä»»åŠ¡ä¼˜å…ˆæ‰§è¡Œ

	// === ç›‘æ§ç»Ÿè®¡ ===
	stats  PoolStats                                // åŸºç¡€ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ´»è·ƒworkerã€å®Œæˆä»»åŠ¡æ•°ç­‰ï¼‰
	name   string                                   // æ± åç§°ï¼Œä¾¿äºæ—¥å¿—è¯†åˆ«å’Œç›‘æ§
	logger func(format string, args ...interface{}) // æ—¥å¿—å‡½æ•°ï¼Œæ”¯æŒè‡ªå®šä¹‰æ—¥å¿—è¾“å‡º

	// === ğŸš€ åŠ¨æ€è°ƒæ•´æ ¸å¿ƒç‰¹æ€§ï¼ˆä¸šç•Œé¢†å…ˆï¼‰ ===
	allowCoreTimeout  bool          // æ˜¯å¦å…è®¸æ ¸å¿ƒworkerè¶…æ—¶é€€å‡º
	keepAliveTime     time.Duration // workerç©ºé—²å¤šä¹…åå¯ä»¥é€€å‡ºï¼ˆä»…åœ¨allowCoreTimeout=trueæ—¶ç”Ÿæ•ˆï¼‰
	lastActivityTime  int64         // æœ€åæ´»åŠ¨æ—¶é—´ï¼ˆçº³ç§’æ—¶é—´æˆ³ï¼‰ï¼Œç”¨äºç©ºé—²æ£€æµ‹
	adjustCheckTicker *time.Ticker  // å®šæ—¶å™¨ï¼Œå®šæœŸæ£€æŸ¥è´Ÿè½½å¹¶è°ƒæ•´CoreWorkers
	stopAdjustCheck   chan struct{} // åœæ­¢è°ƒæ•´æ£€æŸ¥çš„ä¿¡å·é€šé“

	// === ğŸ“Š è´Ÿè½½ç›‘æ§ç³»ç»Ÿ ===
	taskSubmitCount     int64         // åŸå­è®¡æ•°å™¨ï¼Œè®°å½•æ€»æäº¤ä»»åŠ¡æ•°
	loadHistory         []LoadSample  // è´Ÿè½½å†å²è®°å½•ï¼Œæ»‘åŠ¨çª—å£ä¿å­˜æœ€è¿‘3å°æ—¶æ•°æ®
	loadHistoryMu       sync.RWMutex  // è¯»å†™é”ï¼Œä¿æŠ¤è´Ÿè½½å†å²çš„å¹¶å‘è®¿é—®
	adjustCheckInterval time.Duration // è´Ÿè½½æ£€æŸ¥é—´éš”ï¼Œé»˜è®¤10åˆ†é’Ÿ

	// === ğŸ§  æ™ºèƒ½è°ƒæ•´ç­–ç•¥ ===
	coreAdjustStrategy CoreAdjustStrategy // è°ƒæ•´ç­–ç•¥ï¼šå›ºå®š/ç™¾åˆ†æ¯”/æ··åˆ
	lowLoadThreshold   float64            // ä½è´Ÿè½½é˜ˆå€¼(0.0-1.0)ï¼Œä½äºæ­¤å€¼æ—¶è€ƒè™‘ç¼©å®¹
	fixedCoreWorkers   int                // ç”¨æˆ·æ‰‹åŠ¨è®¾ç½®çš„å›ºå®šæ ¸å¿ƒæ•°ï¼Œ0è¡¨ç¤ºä½¿ç”¨åŠ¨æ€è°ƒæ•´
}

// LoadSample è´Ÿè½½é‡‡æ ·æ•°æ®
type LoadSample struct {
	Timestamp     time.Time // é‡‡æ ·æ—¶é—´
	TaskCount     int       // å½“å‰ä»»åŠ¡æ•°é‡
	ActiveWorkers int       // æ´»è·ƒworkeræ•°é‡
	QueueLength   int       // é˜Ÿåˆ—é•¿åº¦
}

// CoreAdjustStrategy æ ¸å¿ƒworkerè°ƒæ•´ç­–ç•¥
type CoreAdjustStrategy int

const (
	// StrategyFixed å›ºå®šç­–ç•¥ï¼šä½¿ç”¨ç”¨æˆ·è®¾å®šçš„å›ºå®šå€¼
	StrategyFixed CoreAdjustStrategy = iota
	// StrategyPercentage ç™¾åˆ†æ¯”ç­–ç•¥ï¼šåŸºäºæœ€è¿‘3å°æ—¶è´Ÿè½½ç™¾åˆ†æ¯”åŠ¨æ€è°ƒæ•´
	StrategyPercentage
	// StrategyHybrid æ··åˆç­–ç•¥ï¼šç»“åˆç™¾åˆ†æ¯”å’Œå›ºå®šå€¼çš„æ™ºèƒ½è°ƒæ•´
	StrategyHybrid
)

// NewPool åˆ›å»ºä¸€ä¸ªåç¨‹æ±  - å·¥å‚å‡½æ•°
// ğŸ“ å‚æ•°è¯´æ˜ï¼š
//
//	minWorkers: æœ€å°workeræ•°é‡ï¼Œå¿…é¡»>=1ï¼Œè¿™äº›workerä¼šç«‹å³å¯åŠ¨å¹¶å¸¸é©»
//	opts: å¯é€‰é…ç½®é¡¹ï¼Œæ”¯æŒé“¾å¼é…ç½®ï¼ˆWithMaxWorkersã€WithLoggerç­‰ï¼‰
//
// ğŸ¯ è®¾è®¡äº®ç‚¹ï¼š
// 1. é‡‡ç”¨Optionsæ¨¡å¼ï¼Œé…ç½®çµæ´»ä¸”å‘åå…¼å®¹
// 2. åˆç†çš„é»˜è®¤å€¼ï¼Œå¼€ç®±å³ç”¨
// 3. ç«‹å³å¯åŠ¨minWorkersä¸ªgoroutineï¼Œç¡®ä¿å“åº”é€Ÿåº¦
// 4. è‡ªåŠ¨å¯åŠ¨è´Ÿè½½ç›‘æ§ï¼Œæ”¯æŒæ™ºèƒ½èµ„æºç®¡ç†
func NewPool(minWorkers int, opts ...PoolOption) *Pool {
	// å‚æ•°æ ¡éªŒï¼šç¡®ä¿è‡³å°‘æœ‰1ä¸ªworker
	if minWorkers < 1 {
		minWorkers = 1
	}

	// åˆ›å»ºPoolå®ä¾‹ï¼Œè®¾ç½®åˆç†çš„é»˜è®¤å€¼
	p := &Pool{
		// === åŸºç¡€é…ç½® ===
		minWorkers:  minWorkers,
		maxWorkers:  minWorkers,                                  // é»˜è®¤æœ€å¤§ç­‰äºæœ€å°ï¼Œå³ä¸è‡ªåŠ¨æ‰©å®¹
		coreWorkers: minWorkers,                                  // é»˜è®¤æ ¸å¿ƒworkerç­‰äºæœ€å°å€¼
		workers:     minWorkers,                                  // å½“å‰workeræ•°é‡
		logger:      func(format string, args ...interface{}) {}, // é»˜è®¤ç©ºæ—¥å¿—å®ç°

		// === ğŸš€ åŠ¨æ€è°ƒæ•´é»˜è®¤é…ç½® ===
		allowCoreTimeout:    false,                      // é»˜è®¤ä¸å…è®¸æ ¸å¿ƒworkerè¶…æ—¶
		keepAliveTime:       60 * time.Second,           // é»˜è®¤60ç§’ç©ºé—²è¶…æ—¶
		adjustCheckInterval: 10 * time.Minute,           // é»˜è®¤10åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡è´Ÿè½½
		stopAdjustCheck:     make(chan struct{}),        // åˆ›å»ºåœæ­¢ä¿¡å·é€šé“
		loadHistory:         make([]LoadSample, 0, 180), // é¢„åˆ†é…å®¹é‡ï¼š3å°æ—¶*æ¯åˆ†é’Ÿ1ä¸ªæ ·æœ¬=180

		// === ğŸ§  æ™ºèƒ½è°ƒæ•´ç­–ç•¥é»˜è®¤é…ç½® ===
		coreAdjustStrategy: StrategyPercentage, // é»˜è®¤ä½¿ç”¨ç™¾åˆ†æ¯”ç­–ç•¥ï¼ˆæ¨èï¼‰
		lowLoadThreshold:   0.3,                // é»˜è®¤30%é˜ˆå€¼ï¼ˆä¿å®ˆè®¾ç½®ï¼‰
		fixedCoreWorkers:   0,                  // 0è¡¨ç¤ºä½¿ç”¨åŠ¨æ€è°ƒæ•´ï¼Œé0è¡¨ç¤ºå›ºå®šå€¼
	}
	// åº”ç”¨ç”¨æˆ·é…ç½®ï¼šOptionsæ¨¡å¼çš„æ ¸å¿ƒï¼Œå…è®¸ç”¨æˆ·è¦†ç›–é»˜è®¤é…ç½®
	for _, opt := range opts {
		opt(p) // æ¯ä¸ªoptionéƒ½æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œä¿®æ”¹Poolçš„ç›¸åº”å­—æ®µ
	}

	// === åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ ===
	p.taskCond = sync.NewCond(&p.mu) // åˆ›å»ºæ¡ä»¶å˜é‡ï¼Œç”¨äºworkeré—´çš„åè°ƒ
	heap.Init(&p.taskQueue)          // åˆå§‹åŒ–ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼Œç¡®ä¿heapæ€§è´¨

	// ğŸš€ ç«‹å³å¯åŠ¨worker goroutines - è¿™æ˜¯æ€§èƒ½çš„å…³é”®ï¼
	// é¢„åˆ†é…workeré¿å…äº†ä»»åŠ¡åˆ°æ¥æ—¶çš„åˆ›å»ºå¼€é”€ï¼Œç¡®ä¿ä½å»¶è¿Ÿå“åº”
	for i := 0; i < minWorkers; i++ {
		p.startWorker() // æ¯ä¸ªworkeréƒ½æ˜¯ç‹¬ç«‹çš„goroutineï¼Œç­‰å¾…ä»»åŠ¡
	}

	// ğŸ”¥ å¯åŠ¨æ™ºèƒ½ç›‘æ§ç³»ç»Ÿ - ä¸šç•Œé¢†å…ˆç‰¹æ€§
	// è¿™ä¸ªgoroutineä¼šå®šæœŸæ”¶é›†è´Ÿè½½æ•°æ®ï¼Œå¹¶æ ¹æ®ç­–ç•¥è°ƒæ•´CoreWorkers
	p.startLoadMonitoring()

	// è®°å½•æ± åˆ›å»ºæ—¥å¿—ï¼Œä¾¿äºè¿ç»´ç›‘æ§
	if p.logger != nil && p.name != "" {
		p.logger("Pool %s created with min=%d, max=%d, core=%d", p.name, p.minWorkers, p.maxWorkers, p.coreWorkers)
	}
	return p
}

// PoolOption ç”¨äºé…ç½® Pool çš„å¯é€‰å‚æ•°
type PoolOption func(*Pool)

// WithMaxWorkers è®¾ç½®æœ€å¤§ worker æ•°
func WithMaxWorkers(max int) PoolOption {
	return func(p *Pool) {
		if max < p.minWorkers {
			max = p.minWorkers
		}
		p.maxWorkers = max
	}
}

// WithName è®¾ç½®æ± çš„åå­—
func WithName(name string) PoolOption {
	return func(p *Pool) {
		p.name = name
	}
}

// WithLogger è®¾ç½®æ± çš„æ—¥å¿—å‡½æ•°
func WithLogger(logger func(format string, args ...interface{})) PoolOption {
	return func(p *Pool) {
		p.logger = logger
	}
}

// WithAllowCoreTimeout å…è®¸æ ¸å¿ƒworkerè¶…æ—¶é€€å‡º
func WithAllowCoreTimeout(allow bool) PoolOption {
	return func(p *Pool) {
		p.allowCoreTimeout = allow
	}
}

// WithKeepAliveTime è®¾ç½®workerç©ºé—²è¶…æ—¶æ—¶é—´
func WithKeepAliveTime(duration time.Duration) PoolOption {
	return func(p *Pool) {
		p.keepAliveTime = duration
	}
}

// WithAdjustCheckInterval è®¾ç½®è°ƒæ•´æ£€æŸ¥é—´éš”
func WithAdjustCheckInterval(interval time.Duration) PoolOption {
	return func(p *Pool) {
		p.adjustCheckInterval = interval
	}
}

// WithCoreAdjustStrategy è®¾ç½®æ ¸å¿ƒworkerè°ƒæ•´ç­–ç•¥
func WithCoreAdjustStrategy(strategy CoreAdjustStrategy) PoolOption {
	return func(p *Pool) {
		p.coreAdjustStrategy = strategy
	}
}

// WithLowLoadThreshold è®¾ç½®ä½è´Ÿè½½ç™¾åˆ†æ¯”é˜ˆå€¼ (0.0-1.0)
func WithLowLoadThreshold(threshold float64) PoolOption {
	return func(p *Pool) {
		if threshold < 0.0 {
			threshold = 0.0
		} else if threshold > 1.0 {
			threshold = 1.0
		}
		p.lowLoadThreshold = threshold
	}
}

// WithFixedCoreWorkers è®¾ç½®å›ºå®šçš„æ ¸å¿ƒworkeræ•°é‡ï¼Œ0è¡¨ç¤ºä½¿ç”¨åŠ¨æ€è°ƒæ•´
func WithFixedCoreWorkers(core int) PoolOption {
	return func(p *Pool) {
		if core < 0 {
			core = 0
		}
		p.fixedCoreWorkers = core
		if core > 0 {
			p.coreAdjustStrategy = StrategyFixed
			p.coreWorkers = core
		}
	}
}

// Submit æäº¤ä»»åŠ¡åˆ°æ± ä¸­ - è¿™æ˜¯æœ€æ ¸å¿ƒçš„API
// ğŸ¯ æ ¸å¿ƒç‰¹æ€§ï¼š
// 1. å¼‚æ­¥æ‰§è¡Œï¼šä»»åŠ¡ç«‹å³å…¥é˜Ÿï¼Œä¸é˜»å¡è°ƒç”¨æ–¹
// 2. ä¼˜å…ˆçº§æ”¯æŒï¼šæ”¯æŒé«˜/ä¸­/ä½ä¼˜å…ˆçº§ä»»åŠ¡è°ƒåº¦
// 3. è¶…æ—¶æ§åˆ¶ï¼šæ”¯æŒContextå’ŒWithTimeoutåŒé‡è¶…æ—¶æœºåˆ¶
// 4. é”™è¯¯æ¢å¤ï¼šæ”¯æŒpanicæ¢å¤ï¼Œä¸å½±å“å…¶ä»–ä»»åŠ¡
// 5. ä¸°å¯Œé€‰é¡¹ï¼šæ”¯æŒæ ‡ç­¾ã€é’©å­ã€æ—¥å¿—ç­‰æ‰©å±•åŠŸèƒ½
//
// ğŸ“ å‚æ•°è¯´æ˜ï¼š
//
//	ctx: ä¸Šä¸‹æ–‡ï¼Œç”¨äºä»»åŠ¡å–æ¶ˆå’Œè¶…æ—¶æ§åˆ¶
//	taskFunc: ä»»åŠ¡å‡½æ•°ï¼Œæ¥æ”¶contextå¹¶è¿”å›ç»“æœå’Œé”™è¯¯
//	opts: å¯é€‰é…ç½®ï¼ˆä¼˜å…ˆçº§ã€è¶…æ—¶ã€æ¢å¤ç­‰ï¼‰
//
// ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼š
// - ä½¿ç”¨åŸå­æ“ä½œè®°å½•ç»Ÿè®¡ä¿¡æ¯ï¼Œé¿å…é”ç«äº‰
// - ä¼˜å…ˆçº§é˜Ÿåˆ—ç¡®ä¿é‡è¦ä»»åŠ¡ä¼˜å…ˆæ‰§è¡Œ
// - è‡ªåŠ¨æ‰©å®¹æœºåˆ¶åº”å¯¹çªå‘æµé‡
func (p *Pool) Submit(ctx context.Context, taskFunc func(ctx context.Context) (interface{}, error), opts ...TaskOption) error {
	// === å‚æ•°æ ¡éªŒ ===
	if taskFunc == nil {
		return fmt.Errorf("taskFunc cannot be nil")
	}

	// === ğŸ“Š ç»Ÿè®¡ä¿¡æ¯æ›´æ–°ï¼ˆåŸå­æ“ä½œï¼Œé«˜æ€§èƒ½ï¼‰ ===
	atomic.StoreInt64(&p.lastActivityTime, time.Now().UnixNano()) // è®°å½•æœ€åæ´»åŠ¨æ—¶é—´ï¼Œç”¨äºç©ºé—²æ£€æµ‹
	atomic.AddInt64(&p.taskSubmitCount, 1)                        // å¢åŠ ä»»åŠ¡è®¡æ•°ï¼Œç”¨äºç›‘æ§

	// === ğŸ”’ ä¸´ç•ŒåŒºï¼šä»»åŠ¡å…¥é˜Ÿæ“ä½œ ===
	p.mu.Lock()
	defer p.mu.Unlock()

	// æ£€æŸ¥æ± çŠ¶æ€ï¼šå¦‚æœæ­£åœ¨å…³é—­ï¼Œæ‹’ç»æ–°ä»»åŠ¡
	if p.shutdown {
		return ErrPoolClosed
	}

	// === ğŸ“¦ æ„é€ ä»»åŠ¡å¯¹è±¡ ===
	task := &Task{
		Priority: PriorityNormal,     // é»˜è®¤æ™®é€šä¼˜å…ˆçº§
		Timeout:  DefaultTaskTimeout, // é»˜è®¤3ç§’è¶…æ—¶
		TaskFunc: taskFunc,           // ç”¨æˆ·ä»»åŠ¡å‡½æ•°
		Ctx:      ctx,                // ä¿å­˜ç”¨æˆ·context
	}

	// åº”ç”¨ç”¨æˆ·é…ç½®é€‰é¡¹ï¼ˆä¼˜å…ˆçº§ã€è¶…æ—¶ã€é’©å­ç­‰ï¼‰
	for _, opt := range opts {
		opt(task)
	}

	// === ğŸ¯ ä»»åŠ¡å…¥é˜Ÿï¼ˆä¼˜å…ˆçº§é˜Ÿåˆ—ï¼‰ ===
	heap.Push(&p.taskQueue, task) // æ ¹æ®ä¼˜å…ˆçº§æ’å…¥é˜Ÿåˆ—
	p.taskCond.Signal()           // å”¤é†’ä¸€ä¸ªç­‰å¾…çš„worker

	// === ğŸš€ è‡ªåŠ¨æ‰©å®¹æ£€æŸ¥ ===
	p.autoScale() // æ ¹æ®é˜Ÿåˆ—é•¿åº¦å†³å®šæ˜¯å¦éœ€è¦åˆ›å»ºæ–°worker

	return nil
}

// SubmitWithResult æäº¤å¸¦è¿”å›å€¼çš„ä»»åŠ¡ï¼Œæ”¯æŒå¯é€‰å‚æ•°
// - å¹¶å‘å®‰å…¨ï¼šå…¨ç¨‹æŒæœ‰é”ï¼Œä¿è¯ä»»åŠ¡é˜Ÿåˆ—å’Œæ± çŠ¶æ€ä¸€è‡´æ€§
// - ctx: æ¨èä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ï¼Œä¾¿äºä»»åŠ¡å–æ¶ˆ/è¶…æ—¶/ç»Ÿä¸€ç®¡ç†
// - Option: æ¨èç”¨ Option ä¼ é€’å¯é€‰å‚æ•°ï¼Œçµæ´»æ‰©å±•
// - é»˜è®¤è¶…æ—¶æ—¶é—´ï¼š3ç§’ï¼Œä¸šåŠ¡å¯ç”¨ WithTimeout è¦†ç›–
func (p *Pool) SubmitWithResult(ctx context.Context, taskFunc func(ctx context.Context) (interface{}, error), opts ...TaskOption) (<-chan TaskResult, error) {
	if taskFunc == nil {
		return nil, fmt.Errorf("taskFunc cannot be nil")
	}

	// è®°å½•æ´»åŠ¨æ—¶é—´å’Œä»»åŠ¡è®¡æ•°
	atomic.StoreInt64(&p.lastActivityTime, time.Now().UnixNano())
	atomic.AddInt64(&p.taskSubmitCount, 1)

	p.mu.Lock()
	defer p.mu.Unlock()
	if p.shutdown {
		return nil, ErrPoolClosed
	}
	resultChan := make(chan TaskResult, 1)
	task := &Task{
		Priority:   PriorityNormal,
		Timeout:    DefaultTaskTimeout, // é»˜è®¤è¶…æ—¶3ç§’ï¼Œä¸šåŠ¡å¯è¦†ç›–
		TaskFunc:   taskFunc,
		ResultChan: resultChan,
		Ctx:        ctx, // æ–°å¢
	}
	for _, opt := range opts {
		opt(task)
	}
	heap.Push(&p.taskQueue, task)
	p.taskCond.Signal()
	p.autoScale()
	return resultChan, nil
}

// startWorker å¯åŠ¨ä¸€ä¸ª worker goroutine
func (p *Pool) startWorker() {
	p.wg.Add(1)
	go func() {
		defer p.wg.Done()
		for {
			p.mu.Lock()
			for len(p.taskQueue) == 0 && !p.shutdown {
				p.taskCond.Wait()
			}
			if p.shutdown && len(p.taskQueue) == 0 {
				p.mu.Unlock()
				return
			}
			task := heap.Pop(&p.taskQueue).(*Task)
			p.stats.ActiveWorkers++
			p.stats.QueuedTasks = len(p.taskQueue)
			p.mu.Unlock()

			// å¤„ç†ä»»åŠ¡ï¼Œctx ç”± Task.Ctx ä¼ é€’
			p.handleTask(task.Ctx, task)

			p.mu.Lock()
			p.stats.ActiveWorkers--
			p.stats.Completed++
			p.stats.QueuedTasks = len(p.taskQueue)
			p.mu.Unlock()
		}
	}()
}

// handleTask å¤„ç†å•ä¸ªä»»åŠ¡ï¼Œctx ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°
func (p *Pool) handleTask(ctx context.Context, task *Task) {
	if ctx == nil {
		ctx = context.Background()
	}
	if task.Timeout > 0 {
		var cancel context.CancelFunc
		ctx, cancel = context.WithTimeout(ctx, task.Timeout)
		defer cancel()
	}
	if task.Before != nil {
		task.Before()
	}
	if task.LogFn != nil {
		task.LogFn("[Task] start tag=%s", task.Tag)
	}
	defer func() {
		if r := recover(); r != nil && task.Recovery != nil {
			task.Recovery(r)
		}
		if task.After != nil {
			task.After()
		}
		if task.LogFn != nil {
			task.LogFn("[Task] end tag=%s", task.Tag)
		}
	}()
	result, err := task.TaskFunc(ctx)
	if task.ResultChan != nil {
		task.ResultChan <- TaskResult{Result: result, Err: err}
		close(task.ResultChan)
	}
}

// autoScale æ ¹æ®ä»»åŠ¡é˜Ÿåˆ—é•¿åº¦è‡ªåŠ¨æ‰©å®¹ worker
func (p *Pool) autoScale() {
	if p.workers >= p.maxWorkers {
		return
	}
	if len(p.taskQueue) > p.workers {
		p.workers++
		p.startWorker()
	}
}

// Stats è¿”å›å½“å‰æ± çš„ç»Ÿè®¡ä¿¡æ¯
func (p *Pool) Stats() PoolStats {
	p.mu.Lock()
	defer p.mu.Unlock()
	return PoolStats{
		ActiveWorkers: p.stats.ActiveWorkers,
		QueuedTasks:   len(p.taskQueue),
		Completed:     p.stats.Completed,
		MinWorkers:    p.minWorkers,
		MaxWorkers:    p.maxWorkers,
	}
}

// Shutdown å…³é—­æ± ï¼Œç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
func (p *Pool) Shutdown() {
	// åœæ­¢è´Ÿè½½ç›‘æ§
	p.mu.Lock()
	if p.stopAdjustCheck != nil {
		select {
		case <-p.stopAdjustCheck:
			// channel already closed
		default:
			close(p.stopAdjustCheck)
		}
	}
	p.shutdown = true
	p.taskCond.Broadcast()
	p.mu.Unlock()
	p.wg.Wait()
}

// SetMinWorkers åŠ¨æ€è®¾ç½®æœ€å° worker æ•°
func (p *Pool) SetMinWorkers(min int) {
	p.mu.Lock()
	defer p.mu.Unlock()
	if min < 1 {
		min = 1
	}
	if min > p.maxWorkers {
		// è‡ªåŠ¨æ‰©å®¹ maxWorkers ä»¥é€‚é…æ›´å¤§çš„ minWorkers
		p.maxWorkers = min
	}
	p.minWorkers = min
	// å¢åŠ  worker
	for p.workers < p.minWorkers {
		p.workers++
		p.startWorker()
	}
}

// SetMaxWorkers åŠ¨æ€è®¾ç½®æœ€å¤§ worker æ•°
func (p *Pool) SetMaxWorkers(max int) {
	p.mu.Lock()
	defer p.mu.Unlock()
	if max < p.minWorkers {
		// è‡ªåŠ¨æ”¶ç¼© minWorkers ä»¥é€‚é…æ›´å°çš„ maxWorkers
		p.minWorkers = max
	}
	p.maxWorkers = max
	// å¦‚æœå½“å‰ worker è¶…è¿‡ maxWorkersï¼Œworker ä¼šåœ¨ç©ºé—²æ—¶è‡ªç„¶é€€å‡º
}

// Restart é‡æ–°å¯åŠ¨æ± ï¼ˆå…³é—­åé‡å¯ï¼‰
func (p *Pool) Restart() {
	p.mu.Lock()
	if !p.shutdown {
		p.mu.Unlock()
		return
	}
	p.shutdown = false
	p.stats = PoolStats{}
	p.workers = p.minWorkers
	heap.Init(&p.taskQueue)
	p.taskCond = sync.NewCond(&p.mu)
	for i := 0; i < p.minWorkers; i++ {
		p.startWorker()
	}
	p.mu.Unlock()
}

// startLoadMonitoring å¯åŠ¨è´Ÿè½½ç›‘æ§å’ŒåŠ¨æ€è°ƒæ•´
func (p *Pool) startLoadMonitoring() {
	if p.adjustCheckInterval <= 0 {
		return // ä¸å¯åŠ¨ç›‘æ§
	}

	p.adjustCheckTicker = time.NewTicker(p.adjustCheckInterval)

	go func() {
		defer p.adjustCheckTicker.Stop()
		for {
			select {
			case <-p.adjustCheckTicker.C:
				p.collectLoadSample()
				p.adjustCoreWorkers()
			case <-p.stopAdjustCheck:
				return
			}
		}
	}()
}

// collectLoadSample æ”¶é›†è´Ÿè½½æ ·æœ¬
func (p *Pool) collectLoadSample() {
	p.mu.Lock()
	sample := LoadSample{
		Timestamp:     time.Now(),
		TaskCount:     int(atomic.LoadInt64(&p.taskSubmitCount)),
		ActiveWorkers: p.stats.ActiveWorkers,
		QueueLength:   len(p.taskQueue),
	}
	p.mu.Unlock()

	p.loadHistoryMu.Lock()
	defer p.loadHistoryMu.Unlock()

	// æ·»åŠ æ–°æ ·æœ¬
	p.loadHistory = append(p.loadHistory, sample)

	// ä¿æŒæœ€è¿‘3å°æ—¶çš„æ•°æ®ï¼ˆ180ä¸ªæ ·æœ¬ï¼Œæ¯åˆ†é’Ÿä¸€ä¸ªï¼‰
	maxSamples := int(3 * time.Hour / p.adjustCheckInterval)
	if len(p.loadHistory) > maxSamples {
		p.loadHistory = p.loadHistory[len(p.loadHistory)-maxSamples:]
	}
}

// adjustCoreWorkers æ ¹æ®ç­–ç•¥è°ƒæ•´æ ¸å¿ƒworkeræ•°é‡
func (p *Pool) adjustCoreWorkers() {
	if p.coreAdjustStrategy == StrategyFixed {
		return // å›ºå®šç­–ç•¥ä¸è°ƒæ•´
	}

	p.loadHistoryMu.RLock()
	historyLen := len(p.loadHistory)
	p.loadHistoryMu.RUnlock()

	if historyLen < 10 { // è‡³å°‘éœ€è¦10ä¸ªæ ·æœ¬
		return
	}

	switch p.coreAdjustStrategy {
	case StrategyPercentage:
		p.adjustByPercentage()
	case StrategyHybrid:
		p.adjustByHybrid()
	}
}

// adjustByPercentage åŸºäºç™¾åˆ†æ¯”ç­–ç•¥è°ƒæ•´
func (p *Pool) adjustByPercentage() {
	p.loadHistoryMu.RLock()
	defer p.loadHistoryMu.RUnlock()

	// è®¡ç®—æœ€è¿‘3å°æ—¶å†…ä½è´Ÿè½½çš„ç™¾åˆ†æ¯”
	lowLoadCount := 0
	totalSamples := len(p.loadHistory)

	for _, sample := range p.loadHistory {
		// å¦‚æœæ´»è·ƒworkeræ•°é‡ä½äºminWorkersçš„é˜ˆå€¼ç™¾åˆ†æ¯”ï¼Œè®¤ä¸ºæ˜¯ä½è´Ÿè½½
		threshold := float64(p.minWorkers) * p.lowLoadThreshold
		if float64(sample.ActiveWorkers) < threshold && sample.QueueLength == 0 {
			lowLoadCount++
		}
	}

	lowLoadRatio := float64(lowLoadCount) / float64(totalSamples)

	// å¦‚æœä½è´Ÿè½½æ¯”ä¾‹è¶…è¿‡80%ï¼Œè€ƒè™‘å‡å°‘æ ¸å¿ƒworkeræ•°é‡
	if lowLoadRatio > 0.8 {
		suggestedCore := int(float64(p.minWorkers) * p.lowLoadThreshold)
		if suggestedCore < 1 {
			suggestedCore = 1
		}

		p.mu.Lock()
		if suggestedCore < p.coreWorkers {
			oldCore := p.coreWorkers
			p.coreWorkers = suggestedCore
			if p.logger != nil {
				p.logger("Core workers adjusted from %d to %d (low load ratio: %.2f%%)",
					oldCore, p.coreWorkers, lowLoadRatio*100)
			}
		}
		p.mu.Unlock()
	} else if lowLoadRatio < 0.3 {
		// å¦‚æœä½è´Ÿè½½æ¯”ä¾‹ä½äº30%ï¼Œè€ƒè™‘æ¢å¤æ ¸å¿ƒworkeræ•°é‡
		p.mu.Lock()
		if p.coreWorkers < p.minWorkers {
			oldCore := p.coreWorkers
			p.coreWorkers = p.minWorkers
			if p.logger != nil {
				p.logger("Core workers restored from %d to %d (load increased)",
					oldCore, p.coreWorkers)
			}
		}
		p.mu.Unlock()
	}
}

// adjustByHybrid æ··åˆç­–ç•¥è°ƒæ•´
func (p *Pool) adjustByHybrid() {
	// å…ˆæ‰§è¡Œç™¾åˆ†æ¯”ç­–ç•¥
	p.adjustByPercentage()

	// ç„¶åè€ƒè™‘ç”¨æˆ·è®¾ç½®çš„å›ºå®šå€¼
	if p.fixedCoreWorkers > 0 {
		p.mu.Lock()
		if p.coreWorkers != p.fixedCoreWorkers {
			oldCore := p.coreWorkers
			p.coreWorkers = p.fixedCoreWorkers
			if p.logger != nil {
				p.logger("Core workers set to fixed value from %d to %d",
					oldCore, p.coreWorkers)
			}
		}
		p.mu.Unlock()
	}
}

// SetCoreWorkers æ‰‹åŠ¨è®¾ç½®æ ¸å¿ƒworkeræ•°é‡
func (p *Pool) SetCoreWorkers(core int) {
	p.mu.Lock()
	defer p.mu.Unlock()

	if core < 1 {
		core = 1
	}
	if core > p.maxWorkers {
		core = p.maxWorkers
	}

	oldCore := p.coreWorkers
	p.coreWorkers = core

	if p.logger != nil && oldCore != core {
		p.logger("Core workers manually set from %d to %d", oldCore, core)
	}
}

// GetCoreWorkers è·å–å½“å‰æ ¸å¿ƒworkeræ•°é‡
func (p *Pool) GetCoreWorkers() int {
	p.mu.Lock()
	defer p.mu.Unlock()
	return p.coreWorkers
}

// GetLoadHistory è·å–è´Ÿè½½å†å²ï¼ˆç”¨äºç›‘æ§å’Œè°ƒè¯•ï¼‰
func (p *Pool) GetLoadHistory() []LoadSample {
	p.loadHistoryMu.RLock()
	defer p.loadHistoryMu.RUnlock()

	// è¿”å›å‰¯æœ¬é¿å…å¹¶å‘é—®é¢˜
	history := make([]LoadSample, len(p.loadHistory))
	copy(history, p.loadHistory)
	return history
}

// EnhancedStats å¢å¼ºçš„ç»Ÿè®¡ä¿¡æ¯
type EnhancedStats struct {
	PoolStats
	CoreWorkers        int       `json:"core_workers"`
	LastActivityTime   time.Time `json:"last_activity_time"`
	TaskSubmitCount    int64     `json:"task_submit_count"`
	AllowCoreTimeout   bool      `json:"allow_core_timeout"`
	CoreAdjustStrategy string    `json:"core_adjust_strategy"`
	LowLoadThreshold   float64   `json:"low_load_threshold"`
	LoadHistoryLength  int       `json:"load_history_length"`
}

// EnhancedStats è·å–å¢å¼ºç»Ÿè®¡ä¿¡æ¯
func (p *Pool) EnhancedStats() EnhancedStats {
	p.mu.Lock()
	// ç›´æ¥æ„å»ºPoolStatsï¼Œé¿å…è°ƒç”¨p.Stats()å¯¼è‡´æ­»é”
	baseStats := PoolStats{
		ActiveWorkers: p.stats.ActiveWorkers,
		QueuedTasks:   len(p.taskQueue),
		Completed:     p.stats.Completed,
		MinWorkers:    p.minWorkers,
		MaxWorkers:    p.maxWorkers,
	}
	coreWorkers := p.coreWorkers
	allowCoreTimeout := p.allowCoreTimeout
	strategy := p.coreAdjustStrategy
	threshold := p.lowLoadThreshold
	p.mu.Unlock()

	p.loadHistoryMu.RLock()
	historyLen := len(p.loadHistory)
	p.loadHistoryMu.RUnlock()

	strategyStr := "Unknown"
	switch strategy {
	case StrategyFixed:
		strategyStr = "Fixed"
	case StrategyPercentage:
		strategyStr = "Percentage"
	case StrategyHybrid:
		strategyStr = "Hybrid"
	}

	return EnhancedStats{
		PoolStats:          baseStats,
		CoreWorkers:        coreWorkers,
		LastActivityTime:   time.Unix(0, atomic.LoadInt64(&p.lastActivityTime)),
		TaskSubmitCount:    atomic.LoadInt64(&p.taskSubmitCount),
		AllowCoreTimeout:   allowCoreTimeout,
		CoreAdjustStrategy: strategyStr,
		LowLoadThreshold:   threshold,
		LoadHistoryLength:  historyLen,
	}
}
